{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [29]\n",
      " [30]\n",
      " [31]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [43]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]\n",
      " [48]\n",
      " [49]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [53]\n",
      " [54]\n",
      " [55]\n",
      " [56]\n",
      " [57]\n",
      " [58]\n",
      " [59]\n",
      " [60]\n",
      " [61]\n",
      " [62]]\n",
      "Epoch 1/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2028\n",
      "Epoch 1: loss improved from inf to 1.03040, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0304\n",
      "Epoch 2/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9386\n",
      "Epoch 2: loss improved from 1.03040 to 0.88505, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8851\n",
      "Epoch 3/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8532\n",
      "Epoch 3: loss improved from 0.88505 to 0.83260, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8326\n",
      "Epoch 4/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7700\n",
      "Epoch 4: loss improved from 0.83260 to 0.77364, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7736\n",
      "Epoch 5/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7897\n",
      "Epoch 5: loss improved from 0.77364 to 0.72679, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7268\n",
      "Epoch 6/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6944\n",
      "Epoch 6: loss improved from 0.72679 to 0.64675, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6467\n",
      "Epoch 7/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5160\n",
      "Epoch 7: loss improved from 0.64675 to 0.57324, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5732\n",
      "Epoch 8/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5203\n",
      "Epoch 8: loss improved from 0.57324 to 0.50421, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5042\n",
      "Epoch 9/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3938\n",
      "Epoch 9: loss improved from 0.50421 to 0.41770, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4177\n",
      "Epoch 10/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.3863\n",
      "Epoch 10: loss improved from 0.41770 to 0.35936, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3594\n",
      "Epoch 11/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2383\n",
      "Epoch 11: loss improved from 0.35936 to 0.25402, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2540\n",
      "Epoch 12/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1653\n",
      "Epoch 12: loss improved from 0.25402 to 0.19604, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1960\n",
      "Epoch 13/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1543\n",
      "Epoch 13: loss improved from 0.19604 to 0.14866, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1487\n",
      "Epoch 14/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1500\n",
      "Epoch 14: loss improved from 0.14866 to 0.13092, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1309\n",
      "Epoch 15/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1248\n",
      "Epoch 15: loss improved from 0.13092 to 0.12181, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1218\n",
      "Epoch 16/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1603\n",
      "Epoch 16: loss did not improve from 0.12181\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1373\n",
      "Epoch 17/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1418\n",
      "Epoch 17: loss did not improve from 0.12181\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1467\n",
      "Epoch 18/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1197\n",
      "Epoch 18: loss did not improve from 0.12181\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1315\n",
      "Epoch 19/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1172\n",
      "Epoch 19: loss improved from 0.12181 to 0.11728, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1173\n",
      "Epoch 20/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1030\n",
      "Epoch 20: loss improved from 0.11728 to 0.10150, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1015\n",
      "Epoch 21/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0856\n",
      "Epoch 21: loss improved from 0.10150 to 0.08948, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0895\n",
      "Epoch 22/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0734\n",
      "Epoch 22: loss improved from 0.08948 to 0.07323, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0732\n",
      "Epoch 23/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0947\n",
      "Epoch 23: loss did not improve from 0.07323\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0932\n",
      "Epoch 24/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0786\n",
      "Epoch 24: loss did not improve from 0.07323\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1156\n",
      "Epoch 25/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0735\n",
      "Epoch 25: loss did not improve from 0.07323\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0949\n",
      "Epoch 26/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.1222\n",
      "Epoch 26: loss did not improve from 0.07323\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1169\n",
      "Epoch 27/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0728\n",
      "Epoch 27: loss did not improve from 0.07323\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0891\n",
      "Epoch 28/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0625\n",
      "Epoch 28: loss improved from 0.07323 to 0.07067, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0707\n",
      "Epoch 29/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0942\n",
      "Epoch 29: loss did not improve from 0.07067\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0965\n",
      "Epoch 30/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0690\n",
      "Epoch 30: loss improved from 0.07067 to 0.06949, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0695\n",
      "Epoch 31/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0690\n",
      "Epoch 31: loss did not improve from 0.06949\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0733\n",
      "Epoch 32/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0716\n",
      "Epoch 32: loss improved from 0.06949 to 0.06296, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0630\n",
      "Epoch 33/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0670\n",
      "Epoch 33: loss did not improve from 0.06296\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0632\n",
      "Epoch 34/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0686\n",
      "Epoch 34: loss did not improve from 0.06296\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0672\n",
      "Epoch 35/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0649\n",
      "Epoch 35: loss improved from 0.06296 to 0.05701, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0570\n",
      "Epoch 36/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0682\n",
      "Epoch 36: loss did not improve from 0.05701\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0673\n",
      "Epoch 37/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0564\n",
      "Epoch 37: loss did not improve from 0.05701\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0652\n",
      "Epoch 38/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0654\n",
      "Epoch 38: loss did not improve from 0.05701\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0594\n",
      "Epoch 39/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0547\n",
      "Epoch 39: loss did not improve from 0.05701\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0722\n",
      "Epoch 40/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0469\n",
      "Epoch 40: loss improved from 0.05701 to 0.05314, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0531\n",
      "Epoch 41/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0747\n",
      "Epoch 41: loss did not improve from 0.05314\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0717\n",
      "Epoch 42/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0487\n",
      "Epoch 42: loss did not improve from 0.05314\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0623\n",
      "Epoch 43/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0643\n",
      "Epoch 43: loss did not improve from 0.05314\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0613\n",
      "Epoch 44/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0497\n",
      "Epoch 44: loss improved from 0.05314 to 0.05290, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0529\n",
      "Epoch 45/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0353\n",
      "Epoch 45: loss improved from 0.05290 to 0.04069, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0407\n",
      "Epoch 46/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0559\n",
      "Epoch 46: loss did not improve from 0.04069\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0513\n",
      "Epoch 47/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0393\n",
      "Epoch 47: loss did not improve from 0.04069\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0415\n",
      "Epoch 48/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0488\n",
      "Epoch 48: loss did not improve from 0.04069\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0483\n",
      "Epoch 49/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0412\n",
      "Epoch 49: loss did not improve from 0.04069\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0418\n",
      "Epoch 50/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0388\n",
      "Epoch 50: loss did not improve from 0.04069\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0423\n",
      "Epoch 51/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0409\n",
      "Epoch 51: loss did not improve from 0.04069\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0482\n",
      "Epoch 52/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0375\n",
      "Epoch 52: loss improved from 0.04069 to 0.03634, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0363\n",
      "Epoch 53/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0442\n",
      "Epoch 53: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0554\n",
      "Epoch 54/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0410\n",
      "Epoch 54: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0457\n",
      "Epoch 55/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0415\n",
      "Epoch 55: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0476\n",
      "Epoch 56/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0462\n",
      "Epoch 56: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0454\n",
      "Epoch 57/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0500\n",
      "Epoch 57: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0502\n",
      "Epoch 58/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0490\n",
      "Epoch 58: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0400\n",
      "Epoch 59/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0398\n",
      "Epoch 59: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0371\n",
      "Epoch 60/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0402\n",
      "Epoch 60: loss did not improve from 0.03634\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0377\n",
      "Epoch 61/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0367\n",
      "Epoch 61: loss improved from 0.03634 to 0.03407, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0341\n",
      "Epoch 62/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0342\n",
      "Epoch 62: loss did not improve from 0.03407\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0389\n",
      "Epoch 63/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0359\n",
      "Epoch 63: loss did not improve from 0.03407\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0359\n",
      "Epoch 64/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0418\n",
      "Epoch 64: loss did not improve from 0.03407\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0399\n",
      "Epoch 65/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0339\n",
      "Epoch 65: loss did not improve from 0.03407\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0349\n",
      "Epoch 66/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0385\n",
      "Epoch 66: loss did not improve from 0.03407\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0358\n",
      "Epoch 67/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0369\n",
      "Epoch 67: loss improved from 0.03407 to 0.03222, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0322\n",
      "Epoch 68/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0330\n",
      "Epoch 68: loss did not improve from 0.03222\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0346\n",
      "Epoch 69/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0355\n",
      "Epoch 69: loss did not improve from 0.03222\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0386\n",
      "Epoch 70/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0288\n",
      "Epoch 70: loss improved from 0.03222 to 0.03145, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0315\n",
      "Epoch 71/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0232\n",
      "Epoch 71: loss did not improve from 0.03145\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0411\n",
      "Epoch 72/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0211\n",
      "Epoch 72: loss did not improve from 0.03145\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0375\n",
      "Epoch 73/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0461\n",
      "Epoch 73: loss did not improve from 0.03145\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0378\n",
      "Epoch 74/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0401\n",
      "Epoch 74: loss did not improve from 0.03145\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0418\n",
      "Epoch 75/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0354\n",
      "Epoch 75: loss improved from 0.03145 to 0.03101, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0310\n",
      "Epoch 76/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0378\n",
      "Epoch 76: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0414\n",
      "Epoch 77/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0295\n",
      "Epoch 77: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0372\n",
      "Epoch 78/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0354\n",
      "Epoch 78: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0364\n",
      "Epoch 79/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0449\n",
      "Epoch 79: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0374\n",
      "Epoch 80/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0191\n",
      "Epoch 80: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0383\n",
      "Epoch 81/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0357\n",
      "Epoch 81: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0367\n",
      "Epoch 82/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0453\n",
      "Epoch 82: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0383\n",
      "Epoch 83/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0430\n",
      "Epoch 83: loss did not improve from 0.03101\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0326\n",
      "Epoch 84/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0271\n",
      "Epoch 84: loss improved from 0.03101 to 0.02932, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0293\n",
      "Epoch 85/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0289\n",
      "Epoch 85: loss did not improve from 0.02932\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0311\n",
      "Epoch 86/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0265\n",
      "Epoch 86: loss improved from 0.02932 to 0.02904, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0290\n",
      "Epoch 87/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0333\n",
      "Epoch 87: loss did not improve from 0.02904\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0319\n",
      "Epoch 88/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0242\n",
      "Epoch 88: loss did not improve from 0.02904\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0292\n",
      "Epoch 89/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0306\n",
      "Epoch 89: loss did not improve from 0.02904\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0304\n",
      "Epoch 90/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0325\n",
      "Epoch 90: loss did not improve from 0.02904\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0291\n",
      "Epoch 91/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0322\n",
      "Epoch 91: loss did not improve from 0.02904\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0316\n",
      "Epoch 92/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0273\n",
      "Epoch 92: loss improved from 0.02904 to 0.02899, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0290\n",
      "Epoch 93/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0268\n",
      "Epoch 93: loss improved from 0.02899 to 0.02848, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0285\n",
      "Epoch 94/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0371\n",
      "Epoch 94: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0299\n",
      "Epoch 95/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0192\n",
      "Epoch 95: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0297\n",
      "Epoch 96/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0371\n",
      "Epoch 96: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0321\n",
      "Epoch 97/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0264\n",
      "Epoch 97: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0295\n",
      "Epoch 98/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0280\n",
      "Epoch 98: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0313\n",
      "Epoch 99/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0345\n",
      "Epoch 99: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0307\n",
      "Epoch 100/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0234\n",
      "Epoch 100: loss did not improve from 0.02848\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0311\n",
      "Epoch 101/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0317\n",
      "Epoch 101: loss improved from 0.02848 to 0.02823, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0282\n",
      "Epoch 102/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0351\n",
      "Epoch 102: loss did not improve from 0.02823\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0303\n",
      "Epoch 103/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0253\n",
      "Epoch 103: loss did not improve from 0.02823\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0287\n",
      "Epoch 104/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0296\n",
      "Epoch 104: loss did not improve from 0.02823\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0284\n",
      "Epoch 105/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0357\n",
      "Epoch 105: loss did not improve from 0.02823\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0308\n",
      "Epoch 106/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0266\n",
      "Epoch 106: loss improved from 0.02823 to 0.02744, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0274\n",
      "Epoch 107/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0258\n",
      "Epoch 107: loss did not improve from 0.02744\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0290\n",
      "Epoch 108/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0358\n",
      "Epoch 108: loss did not improve from 0.02744\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0282\n",
      "Epoch 109/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0346\n",
      "Epoch 109: loss did not improve from 0.02744\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0302\n",
      "Epoch 110/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0338\n",
      "Epoch 110: loss did not improve from 0.02744\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0284\n",
      "Epoch 111/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0310\n",
      "Epoch 111: loss did not improve from 0.02744\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0275\n",
      "Epoch 112/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0301\n",
      "Epoch 112: loss improved from 0.02744 to 0.02720, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0272\n",
      "Epoch 113/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0172\n",
      "Epoch 113: loss did not improve from 0.02720\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0324\n",
      "Epoch 114/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0321\n",
      "Epoch 114: loss did not improve from 0.02720\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0286\n",
      "Epoch 115/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0414\n",
      "Epoch 115: loss did not improve from 0.02720\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0351\n",
      "Epoch 116/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0318\n",
      "Epoch 116: loss did not improve from 0.02720\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0310\n",
      "Epoch 117/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0336\n",
      "Epoch 117: loss did not improve from 0.02720\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0308\n",
      "Epoch 118/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0273\n",
      "Epoch 118: loss did not improve from 0.02720\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0299\n",
      "Epoch 119/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0210\n",
      "Epoch 119: loss improved from 0.02720 to 0.02683, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0268\n",
      "Epoch 120/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0381\n",
      "Epoch 120: loss did not improve from 0.02683\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0330\n",
      "Epoch 121/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0268\n",
      "Epoch 121: loss did not improve from 0.02683\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0293\n",
      "Epoch 122/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0243\n",
      "Epoch 122: loss did not improve from 0.02683\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0281\n",
      "Epoch 123/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0222\n",
      "Epoch 123: loss did not improve from 0.02683\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0300\n",
      "Epoch 124/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0193\n",
      "Epoch 124: loss improved from 0.02683 to 0.02602, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0260\n",
      "Epoch 125/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0330\n",
      "Epoch 125: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0315\n",
      "Epoch 126/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0298\n",
      "Epoch 126: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0302\n",
      "Epoch 127/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0323\n",
      "Epoch 127: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0293\n",
      "Epoch 128/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0290\n",
      "Epoch 128: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0281\n",
      "Epoch 129/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0248\n",
      "Epoch 129: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0347\n",
      "Epoch 130/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0280\n",
      "Epoch 130: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0354\n",
      "Epoch 131/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0272\n",
      "Epoch 131: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0318\n",
      "Epoch 132/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0349\n",
      "Epoch 132: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0308\n",
      "Epoch 133/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0347\n",
      "Epoch 133: loss did not improve from 0.02602\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0316\n",
      "Epoch 134/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0273\n",
      "Epoch 134: loss improved from 0.02602 to 0.02550, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0255\n",
      "Epoch 135/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0404\n",
      "Epoch 135: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0360\n",
      "Epoch 136/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0180\n",
      "Epoch 136: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0466\n",
      "Epoch 137/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0321\n",
      "Epoch 137: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0411\n",
      "Epoch 138/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0476\n",
      "Epoch 138: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0398\n",
      "Epoch 139/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0414\n",
      "Epoch 139: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0537\n",
      "Epoch 140/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0246\n",
      "Epoch 140: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0259\n",
      "Epoch 141/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0781\n",
      "Epoch 141: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0569\n",
      "Epoch 142/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0365\n",
      "Epoch 142: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0394\n",
      "Epoch 143/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0298\n",
      "Epoch 143: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0289\n",
      "Epoch 144/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0421\n",
      "Epoch 144: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0404\n",
      "Epoch 145/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0248\n",
      "Epoch 145: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0321\n",
      "Epoch 146/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0318\n",
      "Epoch 146: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0294\n",
      "Epoch 147/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0468\n",
      "Epoch 147: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0403\n",
      "Epoch 148/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0222\n",
      "Epoch 148: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0261\n",
      "Epoch 149/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0474\n",
      "Epoch 149: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0403\n",
      "Epoch 150/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0214\n",
      "Epoch 150: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0273\n",
      "Epoch 151/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0297\n",
      "Epoch 151: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0369\n",
      "Epoch 152/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0295\n",
      "Epoch 152: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0258\n",
      "Epoch 153/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0494\n",
      "Epoch 153: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0395\n",
      "Epoch 154/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0230\n",
      "Epoch 154: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0335\n",
      "Epoch 155/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0315\n",
      "Epoch 155: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0274\n",
      "Epoch 156/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0421\n",
      "Epoch 156: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0357\n",
      "Epoch 157/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0292\n",
      "Epoch 157: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0280\n",
      "Epoch 158/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0269\n",
      "Epoch 158: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0272\n",
      "Epoch 159/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0345\n",
      "Epoch 159: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0283\n",
      "Epoch 160/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0230\n",
      "Epoch 160: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0259\n",
      "Epoch 161/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0314\n",
      "Epoch 161: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0298\n",
      "Epoch 162/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0217\n",
      "Epoch 162: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0268\n",
      "Epoch 163/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0307\n",
      "Epoch 163: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0283\n",
      "Epoch 164/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0234\n",
      "Epoch 164: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0260\n",
      "Epoch 165/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0208\n",
      "Epoch 165: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0298\n",
      "Epoch 166/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0216\n",
      "Epoch 166: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0319\n",
      "Epoch 167/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0424\n",
      "Epoch 167: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0358\n",
      "Epoch 168/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0336\n",
      "Epoch 168: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0331\n",
      "Epoch 169/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0132\n",
      "Epoch 169: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0360\n",
      "Epoch 170/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0293\n",
      "Epoch 170: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0316\n",
      "Epoch 171/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0381\n",
      "Epoch 171: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0336\n",
      "Epoch 172/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0369\n",
      "Epoch 172: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0288\n",
      "Epoch 173/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0347\n",
      "Epoch 173: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0290\n",
      "Epoch 174/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0307\n",
      "Epoch 174: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0265\n",
      "Epoch 175/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0274\n",
      "Epoch 175: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0262\n",
      "Epoch 176/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0223\n",
      "Epoch 176: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0257\n",
      "Epoch 177/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0227\n",
      "Epoch 177: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0264\n",
      "Epoch 178/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0222\n",
      "Epoch 178: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0293\n",
      "Epoch 179/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0248\n",
      "Epoch 179: loss did not improve from 0.02550\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0291\n",
      "Epoch 180/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0253\n",
      "Epoch 180: loss improved from 0.02550 to 0.02415, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0241\n",
      "Epoch 181/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0437\n",
      "Epoch 181: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0335\n",
      "Epoch 182/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0255\n",
      "Epoch 182: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0277\n",
      "Epoch 183/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0276\n",
      "Epoch 183: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0296\n",
      "Epoch 184/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0255\n",
      "Epoch 184: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0252\n",
      "Epoch 185/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0281\n",
      "Epoch 185: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0258\n",
      "Epoch 186/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0218\n",
      "Epoch 186: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0251\n",
      "Epoch 187/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0308\n",
      "Epoch 187: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0252\n",
      "Epoch 188/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0235\n",
      "Epoch 188: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0257\n",
      "Epoch 189/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0279\n",
      "Epoch 189: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0274\n",
      "Epoch 190/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0163\n",
      "Epoch 190: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0294\n",
      "Epoch 191/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0291\n",
      "Epoch 191: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0274\n",
      "Epoch 192/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0202\n",
      "Epoch 192: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0271\n",
      "Epoch 193/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0325\n",
      "Epoch 193: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0264\n",
      "Epoch 194/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0292\n",
      "Epoch 194: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0255\n",
      "Epoch 195/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0265\n",
      "Epoch 195: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0252\n",
      "Epoch 196/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0222\n",
      "Epoch 196: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0262\n",
      "Epoch 197/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0240\n",
      "Epoch 197: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0270\n",
      "Epoch 198/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0191\n",
      "Epoch 198: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0251\n",
      "Epoch 199/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0192\n",
      "Epoch 199: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0295\n",
      "Epoch 200/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0260\n",
      "Epoch 200: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0290\n",
      "Epoch 201/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0332\n",
      "Epoch 201: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0355\n",
      "Epoch 202/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0389\n",
      "Epoch 202: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0300\n",
      "Epoch 203/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0315\n",
      "Epoch 203: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0257\n",
      "Epoch 204/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0284\n",
      "Epoch 204: loss did not improve from 0.02415\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0294\n",
      "Epoch 205/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0193\n",
      "Epoch 205: loss improved from 0.02415 to 0.02291, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0229\n",
      "Epoch 206/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0350\n",
      "Epoch 206: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0359\n",
      "Epoch 207/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0324\n",
      "Epoch 207: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0242\n",
      "Epoch 208/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0444\n",
      "Epoch 208: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0347\n",
      "Epoch 209/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0339\n",
      "Epoch 209: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0276\n",
      "Epoch 210/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0283\n",
      "Epoch 210: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0290\n",
      "Epoch 211/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0207\n",
      "Epoch 211: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0264\n",
      "Epoch 212/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0292\n",
      "Epoch 212: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0254\n",
      "Epoch 213/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0236\n",
      "Epoch 213: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0245\n",
      "Epoch 214/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0338\n",
      "Epoch 214: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0260\n",
      "Epoch 215/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0158\n",
      "Epoch 215: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0241\n",
      "Epoch 216/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0255\n",
      "Epoch 216: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0257\n",
      "Epoch 217/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0276\n",
      "Epoch 217: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0268\n",
      "Epoch 218/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0221\n",
      "Epoch 218: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0244\n",
      "Epoch 219/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0225\n",
      "Epoch 219: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0266\n",
      "Epoch 220/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0185\n",
      "Epoch 220: loss did not improve from 0.02291\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0287\n",
      "Epoch 221/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0191\n",
      "Epoch 221: loss improved from 0.02291 to 0.02275, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0228\n",
      "Epoch 222/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0274\n",
      "Epoch 222: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0303\n",
      "Epoch 223/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0235\n",
      "Epoch 223: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0247\n",
      "Epoch 224/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0162\n",
      "Epoch 224: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0261\n",
      "Epoch 225/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0330\n",
      "Epoch 225: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0279\n",
      "Epoch 226/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0315\n",
      "Epoch 226: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0291\n",
      "Epoch 227/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0246\n",
      "Epoch 227: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0274\n",
      "Epoch 228/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0294\n",
      "Epoch 228: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0258\n",
      "Epoch 229/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0204\n",
      "Epoch 229: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0256\n",
      "Epoch 230/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0244\n",
      "Epoch 230: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0257\n",
      "Epoch 231/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0284\n",
      "Epoch 231: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0261\n",
      "Epoch 232/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0234\n",
      "Epoch 232: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0235\n",
      "Epoch 233/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0285\n",
      "Epoch 233: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0280\n",
      "Epoch 234/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0291\n",
      "Epoch 234: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0237\n",
      "Epoch 235/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0368\n",
      "Epoch 235: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0304\n",
      "Epoch 236/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0307\n",
      "Epoch 236: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0276\n",
      "Epoch 237/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0352\n",
      "Epoch 237: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0272\n",
      "Epoch 238/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0283\n",
      "Epoch 238: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0281\n",
      "Epoch 239/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0289\n",
      "Epoch 239: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0243\n",
      "Epoch 240/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0249\n",
      "Epoch 240: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0286\n",
      "Epoch 241/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0157\n",
      "Epoch 241: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0259\n",
      "Epoch 242/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0286\n",
      "Epoch 242: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0262\n",
      "Epoch 243/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0373\n",
      "Epoch 243: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0290\n",
      "Epoch 244/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0309\n",
      "Epoch 244: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0250\n",
      "Epoch 245/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0189\n",
      "Epoch 245: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0231\n",
      "Epoch 246/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0259\n",
      "Epoch 246: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0229\n",
      "Epoch 247/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0215\n",
      "Epoch 247: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0235\n",
      "Epoch 248/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0192\n",
      "Epoch 248: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0231\n",
      "Epoch 249/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0268\n",
      "Epoch 249: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0237\n",
      "Epoch 250/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0218\n",
      "Epoch 250: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0273\n",
      "Epoch 251/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0312\n",
      "Epoch 251: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0248\n",
      "Epoch 252/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0320\n",
      "Epoch 252: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0295\n",
      "Epoch 253/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0231\n",
      "Epoch 253: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0251\n",
      "Epoch 254/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0205\n",
      "Epoch 254: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0239\n",
      "Epoch 255/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0267\n",
      "Epoch 255: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0237\n",
      "Epoch 256/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0123\n",
      "Epoch 256: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0246\n",
      "Epoch 257/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0224\n",
      "Epoch 257: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0232\n",
      "Epoch 258/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0234\n",
      "Epoch 258: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0238\n",
      "Epoch 259/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0326\n",
      "Epoch 259: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0258\n",
      "Epoch 260/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0167\n",
      "Epoch 260: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0235\n",
      "Epoch 261/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0270\n",
      "Epoch 261: loss did not improve from 0.02275\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0271\n",
      "Epoch 262/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0279\n",
      "Epoch 262: loss improved from 0.02275 to 0.02272, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0227\n",
      "Epoch 263/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0392\n",
      "Epoch 263: loss did not improve from 0.02272\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0294\n",
      "Epoch 264/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0227\n",
      "Epoch 264: loss improved from 0.02272 to 0.02227, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0223\n",
      "Epoch 265/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0223\n",
      "Epoch 265: loss improved from 0.02227 to 0.02186, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0219\n",
      "Epoch 266/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0152\n",
      "Epoch 266: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0230\n",
      "Epoch 267/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0242\n",
      "Epoch 267: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0298\n",
      "Epoch 268/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0112\n",
      "Epoch 268: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0351\n",
      "Epoch 269/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0174\n",
      "Epoch 269: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0244\n",
      "Epoch 270/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0370\n",
      "Epoch 270: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0300\n",
      "Epoch 271/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0263\n",
      "Epoch 271: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0335\n",
      "Epoch 272/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0320\n",
      "Epoch 272: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0277\n",
      "Epoch 273/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0378\n",
      "Epoch 273: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0303\n",
      "Epoch 274/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0258\n",
      "Epoch 274: loss did not improve from 0.02186\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0306\n",
      "Epoch 275/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0144\n",
      "Epoch 275: loss improved from 0.02186 to 0.02093, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0209\n",
      "Epoch 276/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0359\n",
      "Epoch 276: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0343\n",
      "Epoch 277/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0189\n",
      "Epoch 277: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0267\n",
      "Epoch 278/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0249\n",
      "Epoch 278: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0224\n",
      "Epoch 279/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0317\n",
      "Epoch 279: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0255\n",
      "Epoch 280/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0170\n",
      "Epoch 280: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0213\n",
      "Epoch 281/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0249\n",
      "Epoch 281: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0227\n",
      "Epoch 282/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0221\n",
      "Epoch 282: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0253\n",
      "Epoch 283/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0219\n",
      "Epoch 283: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0263\n",
      "Epoch 284/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0220\n",
      "Epoch 284: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0233\n",
      "Epoch 285/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0267\n",
      "Epoch 285: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0244\n",
      "Epoch 286/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0120\n",
      "Epoch 286: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0264\n",
      "Epoch 287/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0315\n",
      "Epoch 287: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0244\n",
      "Epoch 288/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0214\n",
      "Epoch 288: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0293\n",
      "Epoch 289/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0173\n",
      "Epoch 289: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0257\n",
      "Epoch 290/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0282\n",
      "Epoch 290: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0299\n",
      "Epoch 291/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0199\n",
      "Epoch 291: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0227\n",
      "Epoch 292/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0189\n",
      "Epoch 292: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0224\n",
      "Epoch 293/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0144\n",
      "Epoch 293: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0213\n",
      "Epoch 294/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0260\n",
      "Epoch 294: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0264\n",
      "Epoch 295/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0185\n",
      "Epoch 295: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0222\n",
      "Epoch 296/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0218\n",
      "Epoch 296: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0231\n",
      "Epoch 297/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0226\n",
      "Epoch 297: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0253\n",
      "Epoch 298/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0174\n",
      "Epoch 298: loss did not improve from 0.02093\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0301\n",
      "Epoch 299/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0203\n",
      "Epoch 299: loss improved from 0.02093 to 0.01988, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0199\n",
      "Epoch 300/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0288\n",
      "Epoch 300: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0310\n",
      "Epoch 301/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0209\n",
      "Epoch 301: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0224\n",
      "Epoch 302/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0258\n",
      "Epoch 302: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0279\n",
      "Epoch 303/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0187\n",
      "Epoch 303: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0225\n",
      "Epoch 304/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0223\n",
      "Epoch 304: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0258\n",
      "Epoch 305/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0217\n",
      "Epoch 305: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0220\n",
      "Epoch 306/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0248\n",
      "Epoch 306: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0203\n",
      "Epoch 307/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0253\n",
      "Epoch 307: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0227\n",
      "Epoch 308/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0246\n",
      "Epoch 308: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0212\n",
      "Epoch 309/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0183\n",
      "Epoch 309: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0220\n",
      "Epoch 310/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0146\n",
      "Epoch 310: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0204\n",
      "Epoch 311/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0199\n",
      "Epoch 311: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0209\n",
      "Epoch 312/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0267\n",
      "Epoch 312: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0223\n",
      "Epoch 313/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0150\n",
      "Epoch 313: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0205\n",
      "Epoch 314/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0202\n",
      "Epoch 314: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0232\n",
      "Epoch 315/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0235\n",
      "Epoch 315: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0199\n",
      "Epoch 316/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0104\n",
      "Epoch 316: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0199\n",
      "Epoch 317/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0190\n",
      "Epoch 317: loss did not improve from 0.01988\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0218\n",
      "Epoch 318/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0227\n",
      "Epoch 318: loss improved from 0.01988 to 0.01892, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0189\n",
      "Epoch 319/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0285\n",
      "Epoch 319: loss did not improve from 0.01892\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0215\n",
      "Epoch 320/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0191\n",
      "Epoch 320: loss did not improve from 0.01892\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0196\n",
      "Epoch 321/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0203\n",
      "Epoch 321: loss did not improve from 0.01892\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0216\n",
      "Epoch 322/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0183\n",
      "Epoch 322: loss did not improve from 0.01892\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0222\n",
      "Epoch 323/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0116\n",
      "Epoch 323: loss improved from 0.01892 to 0.01861, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0186\n",
      "Epoch 324/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0172\n",
      "Epoch 324: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0213\n",
      "Epoch 325/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0187\n",
      "Epoch 325: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0244\n",
      "Epoch 326/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0130\n",
      "Epoch 326: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0192\n",
      "Epoch 327/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0273\n",
      "Epoch 327: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0289\n",
      "Epoch 328/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0169\n",
      "Epoch 328: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0220\n",
      "Epoch 329/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0180\n",
      "Epoch 329: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0234\n",
      "Epoch 330/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0241\n",
      "Epoch 330: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0236\n",
      "Epoch 331/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0250\n",
      "Epoch 331: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0211\n",
      "Epoch 332/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0174\n",
      "Epoch 332: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0199\n",
      "Epoch 333/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0126\n",
      "Epoch 333: loss did not improve from 0.01861\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0235\n",
      "Epoch 334/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0182\n",
      "Epoch 334: loss improved from 0.01861 to 0.01844, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0184\n",
      "Epoch 335/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0200\n",
      "Epoch 335: loss did not improve from 0.01844\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0205\n",
      "Epoch 336/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0209\n",
      "Epoch 336: loss did not improve from 0.01844\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0259\n",
      "Epoch 337/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0196\n",
      "Epoch 337: loss did not improve from 0.01844\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0211\n",
      "Epoch 338/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0265\n",
      "Epoch 338: loss did not improve from 0.01844\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0270\n",
      "Epoch 339/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0197\n",
      "Epoch 339: loss did not improve from 0.01844\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0197\n",
      "Epoch 340/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0196\n",
      "Epoch 340: loss improved from 0.01844 to 0.01808, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0181\n",
      "Epoch 341/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0203\n",
      "Epoch 341: loss did not improve from 0.01808\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0196\n",
      "Epoch 342/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0190\n",
      "Epoch 342: loss improved from 0.01808 to 0.01764, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0176\n",
      "Epoch 343/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0101\n",
      "Epoch 343: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0229\n",
      "Epoch 344/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0173\n",
      "Epoch 344: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0180\n",
      "Epoch 345/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0279\n",
      "Epoch 345: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0256\n",
      "Epoch 346/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0184\n",
      "Epoch 346: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 347/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0208\n",
      "Epoch 347: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0210\n",
      "Epoch 348/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0197\n",
      "Epoch 348: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0238\n",
      "Epoch 349/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0186\n",
      "Epoch 349: loss did not improve from 0.01764\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0182\n",
      "Epoch 350/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0138\n",
      "Epoch 350: loss improved from 0.01764 to 0.01763, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0176\n",
      "Epoch 351/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0202\n",
      "Epoch 351: loss improved from 0.01763 to 0.01738, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0174\n",
      "Epoch 352/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0224\n",
      "Epoch 352: loss improved from 0.01738 to 0.01712, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0171\n",
      "Epoch 353/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0188\n",
      "Epoch 353: loss did not improve from 0.01712\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0174\n",
      "Epoch 354/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0177\n",
      "Epoch 354: loss improved from 0.01712 to 0.01708, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0171\n",
      "Epoch 355/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0232\n",
      "Epoch 355: loss did not improve from 0.01708\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0186\n",
      "Epoch 356/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0171\n",
      "Epoch 356: loss did not improve from 0.01708\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0179\n",
      "Epoch 357/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0209\n",
      "Epoch 357: loss did not improve from 0.01708\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 358/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0115\n",
      "Epoch 358: loss did not improve from 0.01708\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0180\n",
      "Epoch 359/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0163\n",
      "Epoch 359: loss improved from 0.01708 to 0.01599, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0160\n",
      "Epoch 360/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0172\n",
      "Epoch 360: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0185\n",
      "Epoch 361/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0187\n",
      "Epoch 361: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0170\n",
      "Epoch 362/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0223\n",
      "Epoch 362: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0203\n",
      "Epoch 363/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0198\n",
      "Epoch 363: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0205\n",
      "Epoch 364/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0233\n",
      "Epoch 364: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0182\n",
      "Epoch 365/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0226\n",
      "Epoch 365: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0186\n",
      "Epoch 366/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0202\n",
      "Epoch 366: loss did not improve from 0.01599\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0194\n",
      "Epoch 367/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0204\n",
      "Epoch 367: loss improved from 0.01599 to 0.01593, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0159\n",
      "Epoch 368/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0245\n",
      "Epoch 368: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0186\n",
      "Epoch 369/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0174\n",
      "Epoch 369: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0195\n",
      "Epoch 370/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0196\n",
      "Epoch 370: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0189\n",
      "Epoch 371/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0172\n",
      "Epoch 371: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0167\n",
      "Epoch 372/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0256\n",
      "Epoch 372: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0257\n",
      "Epoch 373/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0230\n",
      "Epoch 373: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0183\n",
      "Epoch 374/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0306\n",
      "Epoch 374: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0221\n",
      "Epoch 375/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0263\n",
      "Epoch 375: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0246\n",
      "Epoch 376/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0159\n",
      "Epoch 376: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0173\n",
      "Epoch 377/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0189\n",
      "Epoch 377: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0207\n",
      "Epoch 378/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0206\n",
      "Epoch 378: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0354\n",
      "Epoch 379/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0141\n",
      "Epoch 379: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0196\n",
      "Epoch 380/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0452\n",
      "Epoch 380: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0343\n",
      "Epoch 381/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0197\n",
      "Epoch 381: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0285\n",
      "Epoch 382/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0121\n",
      "Epoch 382: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0246\n",
      "Epoch 383/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0230\n",
      "Epoch 383: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0216\n",
      "Epoch 384/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0311\n",
      "Epoch 384: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0251\n",
      "Epoch 385/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0133\n",
      "Epoch 385: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0168\n",
      "Epoch 386/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0150\n",
      "Epoch 386: loss did not improve from 0.01593\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0180\n",
      "Epoch 387/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0142\n",
      "Epoch 387: loss improved from 0.01593 to 0.01536, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0154\n",
      "Epoch 388/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0115\n",
      "Epoch 388: loss improved from 0.01536 to 0.01496, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0150\n",
      "Epoch 389/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0157\n",
      "Epoch 389: loss did not improve from 0.01496\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0150\n",
      "Epoch 390/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0187\n",
      "Epoch 390: loss improved from 0.01496 to 0.01403, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0140\n",
      "Epoch 391/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 391: loss did not improve from 0.01403\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0160\n",
      "Epoch 392/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0140\n",
      "Epoch 392: loss did not improve from 0.01403\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0148\n",
      "Epoch 393/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0143\n",
      "Epoch 393: loss did not improve from 0.01403\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 394/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0107\n",
      "Epoch 394: loss did not improve from 0.01403\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0206\n",
      "Epoch 395/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0141\n",
      "Epoch 395: loss improved from 0.01403 to 0.01355, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0135\n",
      "Epoch 396/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0213\n",
      "Epoch 396: loss did not improve from 0.01355\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0203\n",
      "Epoch 397/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0233\n",
      "Epoch 397: loss did not improve from 0.01355\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0180\n",
      "Epoch 398/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0170\n",
      "Epoch 398: loss did not improve from 0.01355\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0177\n",
      "Epoch 399/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0197\n",
      "Epoch 399: loss did not improve from 0.01355\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0202\n",
      "Epoch 400/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0182\n",
      "Epoch 400: loss improved from 0.01355 to 0.01344, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0134\n",
      "Epoch 401/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0163\n",
      "Epoch 401: loss did not improve from 0.01344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0178\n",
      "Epoch 402/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0112\n",
      "Epoch 402: loss did not improve from 0.01344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 403/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0192\n",
      "Epoch 403: loss did not improve from 0.01344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0169\n",
      "Epoch 404/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0122\n",
      "Epoch 404: loss did not improve from 0.01344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 405/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0144\n",
      "Epoch 405: loss did not improve from 0.01344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 406/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0145\n",
      "Epoch 406: loss improved from 0.01344 to 0.01250, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0125\n",
      "Epoch 407/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0141\n",
      "Epoch 407: loss did not improve from 0.01250\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0160\n",
      "Epoch 408/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0147\n",
      "Epoch 408: loss did not improve from 0.01250\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0144\n",
      "Epoch 409/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0167\n",
      "Epoch 409: loss did not improve from 0.01250\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0149\n",
      "Epoch 410/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0183\n",
      "Epoch 410: loss did not improve from 0.01250\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 411/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0114\n",
      "Epoch 411: loss did not improve from 0.01250\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0132\n",
      "Epoch 412/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0141\n",
      "Epoch 412: loss improved from 0.01250 to 0.01224, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0122\n",
      "Epoch 413/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0120\n",
      "Epoch 413: loss did not improve from 0.01224\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0156\n",
      "Epoch 414/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0128\n",
      "Epoch 414: loss did not improve from 0.01224\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0127\n",
      "Epoch 415/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0184\n",
      "Epoch 415: loss did not improve from 0.01224\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0157\n",
      "Epoch 416/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0114\n",
      "Epoch 416: loss did not improve from 0.01224\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0157\n",
      "Epoch 417/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0074\n",
      "Epoch 417: loss did not improve from 0.01224\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0204\n",
      "Epoch 418/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0136\n",
      "Epoch 418: loss improved from 0.01224 to 0.01167, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0117\n",
      "Epoch 419/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0259\n",
      "Epoch 419: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0228\n",
      "Epoch 420/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0198\n",
      "Epoch 420: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0236\n",
      "Epoch 421/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0146\n",
      "Epoch 421: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0242\n",
      "Epoch 422/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0073\n",
      "Epoch 422: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0128\n",
      "Epoch 423/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0250\n",
      "Epoch 423: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0212\n",
      "Epoch 424/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0179\n",
      "Epoch 424: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0142\n",
      "Epoch 425/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0191\n",
      "Epoch 425: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0162\n",
      "Epoch 426/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0166\n",
      "Epoch 426: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0137\n",
      "Epoch 427/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0103\n",
      "Epoch 427: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0148\n",
      "Epoch 428/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0170\n",
      "Epoch 428: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0131\n",
      "Epoch 429/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0128\n",
      "Epoch 429: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0161\n",
      "Epoch 430/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0099\n",
      "Epoch 430: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0122\n",
      "Epoch 431/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0092\n",
      "Epoch 431: loss did not improve from 0.01167\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0142\n",
      "Epoch 432/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0055\n",
      "Epoch 432: loss improved from 0.01167 to 0.01155, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0115\n",
      "Epoch 433/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0117\n",
      "Epoch 433: loss did not improve from 0.01155\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 434/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0153\n",
      "Epoch 434: loss did not improve from 0.01155\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0126\n",
      "Epoch 435/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0137\n",
      "Epoch 435: loss did not improve from 0.01155\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0119\n",
      "Epoch 436/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0159\n",
      "Epoch 436: loss did not improve from 0.01155\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0131\n",
      "Epoch 437/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0086\n",
      "Epoch 437: loss improved from 0.01155 to 0.01141, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0114\n",
      "Epoch 438/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0124\n",
      "Epoch 438: loss did not improve from 0.01141\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0126\n",
      "Epoch 439/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0116\n",
      "Epoch 439: loss did not improve from 0.01141\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0134\n",
      "Epoch 440/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0132\n",
      "Epoch 440: loss did not improve from 0.01141\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0122\n",
      "Epoch 441/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0114\n",
      "Epoch 441: loss did not improve from 0.01141\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0147\n",
      "Epoch 442/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 442: loss improved from 0.01141 to 0.01070, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0107\n",
      "Epoch 443/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0129\n",
      "Epoch 443: loss did not improve from 0.01070\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0109\n",
      "Epoch 444/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0062\n",
      "Epoch 444: loss did not improve from 0.01070\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0127\n",
      "Epoch 445/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0103\n",
      "Epoch 445: loss did not improve from 0.01070\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0115\n",
      "Epoch 446/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0093\n",
      "Epoch 446: loss improved from 0.01070 to 0.01066, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0107\n",
      "Epoch 447/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0136\n",
      "Epoch 447: loss did not improve from 0.01066\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 448/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0104\n",
      "Epoch 448: loss did not improve from 0.01066\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0107\n",
      "Epoch 449/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0111\n",
      "Epoch 449: loss did not improve from 0.01066\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0120\n",
      "Epoch 450/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0139\n",
      "Epoch 450: loss did not improve from 0.01066\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0115\n",
      "Epoch 451/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0124\n",
      "Epoch 451: loss improved from 0.01066 to 0.01011, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0101\n",
      "Epoch 452/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0090\n",
      "Epoch 452: loss did not improve from 0.01011\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0106\n",
      "Epoch 453/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0085\n",
      "Epoch 453: loss did not improve from 0.01011\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0107\n",
      "Epoch 454/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0108\n",
      "Epoch 454: loss improved from 0.01011 to 0.00953, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0095\n",
      "Epoch 455/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0173\n",
      "Epoch 455: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0127\n",
      "Epoch 456/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0125\n",
      "Epoch 456: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0116\n",
      "Epoch 457/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0092\n",
      "Epoch 457: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 458/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0140\n",
      "Epoch 458: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 459/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0103\n",
      "Epoch 459: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0109\n",
      "Epoch 460/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 460: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0098\n",
      "Epoch 461/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0143\n",
      "Epoch 461: loss did not improve from 0.00953\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0124\n",
      "Epoch 462/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0077\n",
      "Epoch 462: loss improved from 0.00953 to 0.00942, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0094\n",
      "Epoch 463/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 463: loss did not improve from 0.00942\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0100\n",
      "Epoch 464/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 464: loss improved from 0.00942 to 0.00918, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0092\n",
      "Epoch 465/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 465: loss did not improve from 0.00918\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0100\n",
      "Epoch 466/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0128\n",
      "Epoch 466: loss did not improve from 0.00918\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0126\n",
      "Epoch 467/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 467: loss did not improve from 0.00918\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0115\n",
      "Epoch 468/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0087\n",
      "Epoch 468: loss improved from 0.00918 to 0.00851, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0085\n",
      "Epoch 469/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0218\n",
      "Epoch 469: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0165\n",
      "Epoch 470/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 470: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0101\n",
      "Epoch 471/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 471: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0098\n",
      "Epoch 472/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0096\n",
      "Epoch 472: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 473/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0127\n",
      "Epoch 473: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 474/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 474: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0127\n",
      "Epoch 475/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 475: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0186\n",
      "Epoch 476/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0080\n",
      "Epoch 476: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0152\n",
      "Epoch 477/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0138\n",
      "Epoch 477: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 478/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0157\n",
      "Epoch 478: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 479/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0120\n",
      "Epoch 479: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0128\n",
      "Epoch 480/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0094\n",
      "Epoch 480: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0128\n",
      "Epoch 481/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0112\n",
      "Epoch 481: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 482/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0144\n",
      "Epoch 482: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0144\n",
      "Epoch 483/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0117\n",
      "Epoch 483: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 484/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0114\n",
      "Epoch 484: loss did not improve from 0.00851\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0105\n",
      "Epoch 485/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0083\n",
      "Epoch 485: loss improved from 0.00851 to 0.00778, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0078\n",
      "Epoch 486/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0128\n",
      "Epoch 486: loss did not improve from 0.00778\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0132\n",
      "Epoch 487/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0093\n",
      "Epoch 487: loss did not improve from 0.00778\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0090\n",
      "Epoch 488/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0112\n",
      "Epoch 488: loss did not improve from 0.00778\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0094\n",
      "Epoch 489/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 489: loss did not improve from 0.00778\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0111\n",
      "Epoch 490/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0085\n",
      "Epoch 490: loss did not improve from 0.00778\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0085\n",
      "Epoch 491/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0096\n",
      "Epoch 491: loss improved from 0.00778 to 0.00773, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0077\n",
      "Epoch 492/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0091\n",
      "Epoch 492: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 493/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0113\n",
      "Epoch 493: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0099\n",
      "Epoch 494/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0118\n",
      "Epoch 494: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 495/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 495: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0121\n",
      "Epoch 496/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 496: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0084\n",
      "Epoch 497/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0121\n",
      "Epoch 497: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0095\n",
      "Epoch 498/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0152\n",
      "Epoch 498: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0112\n",
      "Epoch 499/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 499: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 500/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 500: loss did not improve from 0.00773\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0097\n",
      "Epoch 501/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 501: loss improved from 0.00773 to 0.00749, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0075\n",
      "Epoch 502/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0092\n",
      "Epoch 502: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0098\n",
      "Epoch 503/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 503: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 504/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0074\n",
      "Epoch 504: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 505/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 505: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 506/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0098\n",
      "Epoch 506: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 507/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0099\n",
      "Epoch 507: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 508/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 508: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 509/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0074\n",
      "Epoch 509: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 510/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0088\n",
      "Epoch 510: loss did not improve from 0.00749\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0087\n",
      "Epoch 511/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 511: loss improved from 0.00749 to 0.00713, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0071\n",
      "Epoch 512/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0079\n",
      "Epoch 512: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 513/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 513: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 514/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 514: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 515/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 515: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0086\n",
      "Epoch 516/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0098\n",
      "Epoch 516: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 517/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0091\n",
      "Epoch 517: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 518/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 518: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 519/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0096\n",
      "Epoch 519: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 520/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0055\n",
      "Epoch 520: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 521/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 521: loss did not improve from 0.00713\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 522/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 522: loss improved from 0.00713 to 0.00710, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0071\n",
      "Epoch 523/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0080\n",
      "Epoch 523: loss did not improve from 0.00710\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0078\n",
      "Epoch 524/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0067\n",
      "Epoch 524: loss improved from 0.00710 to 0.00679, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0068\n",
      "Epoch 525/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 525: loss improved from 0.00679 to 0.00668, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0067\n",
      "Epoch 526/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 526: loss improved from 0.00668 to 0.00646, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0065\n",
      "Epoch 527/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0097\n",
      "Epoch 527: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0079\n",
      "Epoch 528/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 528: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 529/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 529: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 530/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0076\n",
      "Epoch 530: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 531/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0168\n",
      "Epoch 531: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0118\n",
      "Epoch 532/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 532: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0077\n",
      "Epoch 533/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0066\n",
      "Epoch 533: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 534/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0079\n",
      "Epoch 534: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 535/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 535: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0080\n",
      "Epoch 536/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 536: loss did not improve from 0.00646\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0076\n",
      "Epoch 537/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 537: loss improved from 0.00646 to 0.00633, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0063\n",
      "Epoch 538/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0089\n",
      "Epoch 538: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0064\n",
      "Epoch 539/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0086\n",
      "Epoch 539: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 540/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 540: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0068\n",
      "Epoch 541/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 541: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 542/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0099\n",
      "Epoch 542: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0115\n",
      "Epoch 543/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0070\n",
      "Epoch 543: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0132\n",
      "Epoch 544/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0077\n",
      "Epoch 544: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0109\n",
      "Epoch 545/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0145\n",
      "Epoch 545: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0120\n",
      "Epoch 546/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0141\n",
      "Epoch 546: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0158\n",
      "Epoch 547/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0182\n",
      "Epoch 547: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0223\n",
      "Epoch 548/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0120\n",
      "Epoch 548: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0182\n",
      "Epoch 549/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 549: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0097\n",
      "Epoch 550/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0236\n",
      "Epoch 550: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0179\n",
      "Epoch 551/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0109\n",
      "Epoch 551: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0131\n",
      "Epoch 552/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0103\n",
      "Epoch 552: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0205\n",
      "Epoch 553/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0066\n",
      "Epoch 553: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0180\n",
      "Epoch 554/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0132\n",
      "Epoch 554: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0172\n",
      "Epoch 555/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0289\n",
      "Epoch 555: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0200\n",
      "Epoch 556/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0266\n",
      "Epoch 556: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0263\n",
      "Epoch 557/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0216\n",
      "Epoch 557: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0270\n",
      "Epoch 558/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0091\n",
      "Epoch 558: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 559/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0212\n",
      "Epoch 559: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 560/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0136\n",
      "Epoch 560: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0261\n",
      "Epoch 561/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0084\n",
      "Epoch 561: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0121\n",
      "Epoch 562/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0199\n",
      "Epoch 562: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0195\n",
      "Epoch 563/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0175\n",
      "Epoch 563: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0239\n",
      "Epoch 564/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0090\n",
      "Epoch 564: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0119\n",
      "Epoch 565/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0280\n",
      "Epoch 565: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 566/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0274\n",
      "Epoch 566: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0222\n",
      "Epoch 567/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 567: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0118\n",
      "Epoch 568/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0138\n",
      "Epoch 568: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0101\n",
      "Epoch 569/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0150\n",
      "Epoch 569: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0162\n",
      "Epoch 570/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 570: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0091\n",
      "Epoch 571/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0124\n",
      "Epoch 571: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 572/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0120\n",
      "Epoch 572: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0130\n",
      "Epoch 573/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0089\n",
      "Epoch 573: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 574/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0117\n",
      "Epoch 574: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0094\n",
      "Epoch 575/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0136\n",
      "Epoch 575: loss did not improve from 0.00633\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0135\n",
      "Epoch 576/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 576: loss improved from 0.00633 to 0.00628, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0063\n",
      "Epoch 577/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0136\n",
      "Epoch 577: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0125\n",
      "Epoch 578/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0131\n",
      "Epoch 578: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0104\n",
      "Epoch 579/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0103\n",
      "Epoch 579: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 580/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 580: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0095\n",
      "Epoch 581/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 581: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 582/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 582: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0094\n",
      "Epoch 583/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 583: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0095\n",
      "Epoch 584/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 584: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 585/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0081\n",
      "Epoch 585: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 586/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0060\n",
      "Epoch 586: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0097\n",
      "Epoch 587/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0066\n",
      "Epoch 587: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 588/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0117\n",
      "Epoch 588: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 589/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0088\n",
      "Epoch 589: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0100\n",
      "Epoch 590/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0042\n",
      "Epoch 590: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 591/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 591: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 592/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 592: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 593/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0084\n",
      "Epoch 593: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0075\n",
      "Epoch 594/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0088\n",
      "Epoch 594: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0063\n",
      "Epoch 595/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0083\n",
      "Epoch 595: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0066\n",
      "Epoch 596/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0062\n",
      "Epoch 596: loss did not improve from 0.00628\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 597/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 597: loss improved from 0.00628 to 0.00562, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0056\n",
      "Epoch 598/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 598: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 599/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 599: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 600/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 600: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 601/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 601: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 602/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 602: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 603/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 603: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 604/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 604: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 605/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 605: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 606/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0066\n",
      "Epoch 606: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 607/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0067\n",
      "Epoch 607: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 608/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 608: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 609/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 609: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 610/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0074\n",
      "Epoch 610: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 611/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 611: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 612/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 612: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 613/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 613: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 614/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 614: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 615/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 615: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 616/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 616: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 617/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0080\n",
      "Epoch 617: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0084\n",
      "Epoch 618/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 618: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 619/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 619: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 620/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0067\n",
      "Epoch 620: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 621/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0087\n",
      "Epoch 621: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 622/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 622: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 623/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 623: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 624/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0089\n",
      "Epoch 624: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 625/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 625: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 626/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0067\n",
      "Epoch 626: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 627/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 627: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 628/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0050\n",
      "Epoch 628: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 629/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0077\n",
      "Epoch 629: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 630/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0077\n",
      "Epoch 630: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 631/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 631: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 632/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0070\n",
      "Epoch 632: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 633/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 633: loss did not improve from 0.00562\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 634/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 634: loss improved from 0.00562 to 0.00540, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0054\n",
      "Epoch 635/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0029\n",
      "Epoch 635: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 636/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 636: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 637/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 637: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 638/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 638: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 639/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 639: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 640/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 640: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 641/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 641: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 642/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0029\n",
      "Epoch 642: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 643/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0107\n",
      "Epoch 643: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 644/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 644: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 645/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 645: loss did not improve from 0.00540\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 646/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 646: loss improved from 0.00540 to 0.00493, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0049\n",
      "Epoch 647/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0062\n",
      "Epoch 647: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 648/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 648: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 649/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 649: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 650/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0067\n",
      "Epoch 650: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0073\n",
      "Epoch 651/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 651: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0078\n",
      "Epoch 652/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 652: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 653/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 653: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 654/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0079\n",
      "Epoch 654: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0070\n",
      "Epoch 655/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0025\n",
      "Epoch 655: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 656/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 656: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 657/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0075\n",
      "Epoch 657: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 658/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 658: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0051\n",
      "Epoch 659/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 659: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 660/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 660: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 661/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 661: loss did not improve from 0.00493\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 662/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0028\n",
      "Epoch 662: loss improved from 0.00493 to 0.00460, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0046\n",
      "Epoch 663/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 663: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 664/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 664: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 665/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 665: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0083\n",
      "Epoch 666/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 666: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0107\n",
      "Epoch 667/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 667: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 668/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 668: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 669/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 669: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 670/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 670: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 671/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 671: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 672/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 672: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0080\n",
      "Epoch 673/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0077\n",
      "Epoch 673: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0070\n",
      "Epoch 674/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 674: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 675/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 675: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0064\n",
      "Epoch 676/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 676: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 677/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0056\n",
      "Epoch 677: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0049\n",
      "Epoch 678/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 678: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 679/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 679: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 680/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 680: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 681/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 681: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 682/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 682: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 683/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0090\n",
      "Epoch 683: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 684/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0051\n",
      "Epoch 684: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 685/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 685: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 686/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 686: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 687/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0075\n",
      "Epoch 687: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0071\n",
      "Epoch 688/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0103\n",
      "Epoch 688: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 689/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 689: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 690/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0033\n",
      "Epoch 690: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 691/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0104\n",
      "Epoch 691: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 692/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0027\n",
      "Epoch 692: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 693/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034\n",
      "Epoch 693: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 694/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 694: loss did not improve from 0.00460\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 695/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 695: loss improved from 0.00460 to 0.00449, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0045\n",
      "Epoch 696/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0082\n",
      "Epoch 696: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 697/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 697: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 698/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0023\n",
      "Epoch 698: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 699/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 699: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 700/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 700: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 701/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0070\n",
      "Epoch 701: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0076\n",
      "Epoch 702/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 702: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 703/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 703: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 704/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0051\n",
      "Epoch 704: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 705/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0076\n",
      "Epoch 705: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 706/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 706: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 707/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 707: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 708/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 708: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0086\n",
      "Epoch 709/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0107\n",
      "Epoch 709: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0083\n",
      "Epoch 710/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 710: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 711/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 711: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0120\n",
      "Epoch 712/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 712: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 713/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 713: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 714/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0119\n",
      "Epoch 714: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0122\n",
      "Epoch 715/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 715: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0099\n",
      "Epoch 716/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0033\n",
      "Epoch 716: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 717/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0088\n",
      "Epoch 717: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 718/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0078\n",
      "Epoch 718: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 719/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 719: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 720/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 720: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 721/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0123\n",
      "Epoch 721: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0102\n",
      "Epoch 722/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0137\n",
      "Epoch 722: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 723/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0028\n",
      "Epoch 723: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 724/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0055\n",
      "Epoch 724: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 725/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0073\n",
      "Epoch 725: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 726/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 726: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0076\n",
      "Epoch 727/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 727: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0047\n",
      "Epoch 728/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 728: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 729/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 729: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 730/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 730: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 731/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 731: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 732/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0019\n",
      "Epoch 732: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 733/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0033\n",
      "Epoch 733: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 734/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 734: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 735/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 735: loss did not improve from 0.00449\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 736/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0043\n",
      "Epoch 736: loss improved from 0.00449 to 0.00438, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0044\n",
      "Epoch 737/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0048\n",
      "Epoch 737: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 738/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 738: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 739/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 739: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 740/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 740: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 741/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 741: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 742/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0042\n",
      "Epoch 742: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 743/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 743: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 744/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0023\n",
      "Epoch 744: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 745/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0085\n",
      "Epoch 745: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 746/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 746: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 747/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0076\n",
      "Epoch 747: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 748/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 748: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 749/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0050\n",
      "Epoch 749: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 750/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0062\n",
      "Epoch 750: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 751/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 751: loss did not improve from 0.00438\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 752/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 752: loss improved from 0.00438 to 0.00412, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0041\n",
      "Epoch 753/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 753: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 754/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 754: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 755/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 755: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 756/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 756: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 757/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 757: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 758/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 758: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 759/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0105\n",
      "Epoch 759: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 760/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0076\n",
      "Epoch 760: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 761/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 761: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 762/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 762: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 763/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 763: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 764/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0056\n",
      "Epoch 764: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 765/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0125\n",
      "Epoch 765: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 766/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0116\n",
      "Epoch 766: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 767/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 767: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 768/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 768: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 769/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0076\n",
      "Epoch 769: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0084\n",
      "Epoch 770/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0060\n",
      "Epoch 770: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 771/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0094\n",
      "Epoch 771: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 772/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 772: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 773/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 773: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 774/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 774: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 775/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 775: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 776/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 776: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 777/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 777: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 778/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 778: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 779/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0076\n",
      "Epoch 779: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0069\n",
      "Epoch 780/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 780: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0045\n",
      "Epoch 781/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0056\n",
      "Epoch 781: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 782/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 782: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 783/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 783: loss did not improve from 0.00412\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 784/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 784: loss improved from 0.00412 to 0.00383, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0038\n",
      "Epoch 785/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 785: loss did not improve from 0.00383\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 786/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 786: loss did not improve from 0.00383\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0043\n",
      "Epoch 787/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 787: loss improved from 0.00383 to 0.00381, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0038\n",
      "Epoch 788/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 788: loss did not improve from 0.00381\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 789/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 789: loss did not improve from 0.00381\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0043\n",
      "Epoch 790/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0033\n",
      "Epoch 790: loss improved from 0.00381 to 0.00359, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0036\n",
      "Epoch 791/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0022\n",
      "Epoch 791: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0043\n",
      "Epoch 792/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034\n",
      "Epoch 792: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 793/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0043\n",
      "Epoch 793: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 794/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0048\n",
      "Epoch 794: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 795/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 795: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 796/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 796: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 797/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 797: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 798/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0042\n",
      "Epoch 798: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 799/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 799: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 800/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 800: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 801/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0029\n",
      "Epoch 801: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 802/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 802: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 803/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0021\n",
      "Epoch 803: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 804/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 804: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 805/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0102\n",
      "Epoch 805: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 806/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0115\n",
      "Epoch 806: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 807/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0077\n",
      "Epoch 807: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 808/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0050\n",
      "Epoch 808: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0044\n",
      "Epoch 809/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 809: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 810/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 810: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 811/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 811: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 812/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0060\n",
      "Epoch 812: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 813/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 813: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 814/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 814: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0069\n",
      "Epoch 815/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0031\n",
      "Epoch 815: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0087\n",
      "Epoch 816/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 816: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0070\n",
      "Epoch 817/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0043\n",
      "Epoch 817: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0041\n",
      "Epoch 818/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0200\n",
      "Epoch 818: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0136\n",
      "Epoch 819/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0113\n",
      "Epoch 819: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0111\n",
      "Epoch 820/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 820: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0114\n",
      "Epoch 821/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 821: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0120\n",
      "Epoch 822/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 822: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 823/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0193\n",
      "Epoch 823: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0133\n",
      "Epoch 824/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0110\n",
      "Epoch 824: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0108\n",
      "Epoch 825/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0110\n",
      "Epoch 825: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0094\n",
      "Epoch 826/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 826: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 827/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0141\n",
      "Epoch 827: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0118\n",
      "Epoch 828/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0137\n",
      "Epoch 828: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0133\n",
      "Epoch 829/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 829: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0067\n",
      "Epoch 830/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0055\n",
      "Epoch 830: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 831/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0150\n",
      "Epoch 831: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0116\n",
      "Epoch 832/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0110\n",
      "Epoch 832: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 833/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 833: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 834/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0080\n",
      "Epoch 834: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 835/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0120\n",
      "Epoch 835: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0087\n",
      "Epoch 836/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0139\n",
      "Epoch 836: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0120\n",
      "Epoch 837/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 837: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 838/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 838: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 839/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0161\n",
      "Epoch 839: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0130\n",
      "Epoch 840/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0127\n",
      "Epoch 840: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 841/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 841: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0087\n",
      "Epoch 842/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 842: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0085\n",
      "Epoch 843/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 843: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 844/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0104\n",
      "Epoch 844: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0082\n",
      "Epoch 845/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0070\n",
      "Epoch 845: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 846/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 846: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 847/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 847: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 848/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0060\n",
      "Epoch 848: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 849/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 849: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 850/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 850: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0042\n",
      "Epoch 851/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 851: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 852/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0051\n",
      "Epoch 852: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 853/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0043\n",
      "Epoch 853: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 854/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 854: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 855/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 855: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 856/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 856: loss did not improve from 0.00359\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 857/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 857: loss improved from 0.00359 to 0.00345, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0035\n",
      "Epoch 858/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 858: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 859/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 859: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 860/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034\n",
      "Epoch 860: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 861/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 861: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 862/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0090\n",
      "Epoch 862: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 863/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 863: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 864/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0029\n",
      "Epoch 864: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 865/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0023\n",
      "Epoch 865: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 866/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 866: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 867/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 867: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 868/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 868: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0035\n",
      "Epoch 869/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 869: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0037\n",
      "Epoch 870/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 870: loss did not improve from 0.00345\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 871/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0017\n",
      "Epoch 871: loss improved from 0.00345 to 0.00344, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0034\n",
      "Epoch 872/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0031\n",
      "Epoch 872: loss did not improve from 0.00344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 873/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0050\n",
      "Epoch 873: loss did not improve from 0.00344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 874/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 874: loss did not improve from 0.00344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 875/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 875: loss did not improve from 0.00344\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 876/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 876: loss improved from 0.00344 to 0.00341, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0034\n",
      "Epoch 877/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 877: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 878/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0022\n",
      "Epoch 878: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 879/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 879: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 880/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 880: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 881/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 881: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 882/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 882: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 883/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 883: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0051\n",
      "Epoch 884/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0050\n",
      "Epoch 884: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 885/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 885: loss did not improve from 0.00341\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 886/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 886: loss improved from 0.00341 to 0.00322, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0032\n",
      "Epoch 887/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0059\n",
      "Epoch 887: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0049\n",
      "Epoch 888/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 888: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 889/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 889: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 890/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 890: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 891/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0053\n",
      "Epoch 891: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 892/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 892: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 893/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0072\n",
      "Epoch 893: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 894/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 894: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "Epoch 895/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 895: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0034\n",
      "Epoch 896/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 896: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 897/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0023\n",
      "Epoch 897: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0034\n",
      "Epoch 898/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0027\n",
      "Epoch 898: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0055\n",
      "Epoch 899/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 899: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 900/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 900: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 901/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0081\n",
      "Epoch 901: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 902/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 902: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0056\n",
      "Epoch 903/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 903: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 904/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 904: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 905/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 905: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 906/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034\n",
      "Epoch 906: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "Epoch 907/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0071\n",
      "Epoch 907: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 908/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 908: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 909/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 909: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 910/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 910: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 911/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 911: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 912/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0042\n",
      "Epoch 912: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 913/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 913: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 914/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 914: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 915/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 915: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 916/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 916: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 917/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0024\n",
      "Epoch 917: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0039\n",
      "Epoch 918/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0018\n",
      "Epoch 918: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0047\n",
      "Epoch 919/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 919: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0035\n",
      "Epoch 920/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 920: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 921/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 921: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 922/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0061\n",
      "Epoch 922: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 923/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 923: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 924/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0035\n",
      "Epoch 924: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 925/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0123\n",
      "Epoch 925: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0101\n",
      "Epoch 926/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 926: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 927/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0037\n",
      "Epoch 927: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0044\n",
      "Epoch 928/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 928: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0036\n",
      "Epoch 929/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0063\n",
      "Epoch 929: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 930/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0057\n",
      "Epoch 930: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 931/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0031\n",
      "Epoch 931: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0032\n",
      "Epoch 932/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034\n",
      "Epoch 932: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0035\n",
      "Epoch 933/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0031\n",
      "Epoch 933: loss did not improve from 0.00322\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0034\n",
      "Epoch 934/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 934: loss improved from 0.00322 to 0.00306, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0031\n",
      "Epoch 935/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0028\n",
      "Epoch 935: loss improved from 0.00306 to 0.00298, saving model to best_model.h5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0030\n",
      "Epoch 936/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0020\n",
      "Epoch 936: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0034\n",
      "Epoch 937/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0032\n",
      "Epoch 937: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 938/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0022\n",
      "Epoch 938: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0049\n",
      "Epoch 939/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0028\n",
      "Epoch 939: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0051\n",
      "Epoch 940/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0033\n",
      "Epoch 940: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0071\n",
      "Epoch 941/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 941: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 942/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0068\n",
      "Epoch 942: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 943/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0067\n",
      "Epoch 943: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 944/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0065\n",
      "Epoch 944: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 945/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 945: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 946/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0095\n",
      "Epoch 946: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0077\n",
      "Epoch 947/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 947: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0070\n",
      "Epoch 948/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0034\n",
      "Epoch 948: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 949/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0029\n",
      "Epoch 949: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0042\n",
      "Epoch 950/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0060\n",
      "Epoch 950: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 951/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0100\n",
      "Epoch 951: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 952/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0047\n",
      "Epoch 952: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0038\n",
      "Epoch 953/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0022\n",
      "Epoch 953: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "Epoch 954/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 954: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 955/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0042\n",
      "Epoch 955: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 956/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0020\n",
      "Epoch 956: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 957/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0027\n",
      "Epoch 957: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 958/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 958: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0034\n",
      "Epoch 959/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0081\n",
      "Epoch 959: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0069\n",
      "Epoch 960/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 960: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0038\n",
      "Epoch 961/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0036\n",
      "Epoch 961: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 962/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0056\n",
      "Epoch 962: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 963/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 963: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 964/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0060\n",
      "Epoch 964: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0052\n",
      "Epoch 965/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0177\n",
      "Epoch 965: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0152\n",
      "Epoch 966/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0090\n",
      "Epoch 966: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0086\n",
      "Epoch 967/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 967: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 968/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0091\n",
      "Epoch 968: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 969/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0112\n",
      "Epoch 969: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0125\n",
      "Epoch 970/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 970: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0121\n",
      "Epoch 971/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0026\n",
      "Epoch 971: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 972/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0112\n",
      "Epoch 972: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0097\n",
      "Epoch 973/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0151\n",
      "Epoch 973: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 974/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0096\n",
      "Epoch 974: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0079\n",
      "Epoch 975/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0044\n",
      "Epoch 975: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 976/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0046\n",
      "Epoch 976: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 977/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0090\n",
      "Epoch 977: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 978/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0054\n",
      "Epoch 978: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 979/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 979: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 980/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0098\n",
      "Epoch 980: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0087\n",
      "Epoch 981/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0073\n",
      "Epoch 981: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 982/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 982: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 983/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0058\n",
      "Epoch 983: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 984/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 984: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0070\n",
      "Epoch 985/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0045\n",
      "Epoch 985: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 986/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0038\n",
      "Epoch 986: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 987/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0096\n",
      "Epoch 987: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 988/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0084\n",
      "Epoch 988: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 989/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0019\n",
      "Epoch 989: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 990/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0041\n",
      "Epoch 990: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 991/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0069\n",
      "Epoch 991: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 992/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0086\n",
      "Epoch 992: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 993/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0023\n",
      "Epoch 993: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 994/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0040\n",
      "Epoch 994: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 995/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 995: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 996/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0049\n",
      "Epoch 996: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 997/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0022\n",
      "Epoch 997: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 998/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 998: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 999/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0052\n",
      "Epoch 999: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 1000/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0039\n",
      "Epoch 1000: loss did not improve from 0.00298\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0043\n",
      "Best model is from epoch 935 and loss is 0.0029845864046365023\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "Predicted number of faults: [61.452763]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAIhCAYAAAAy8fsSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAACaqUlEQVR4nOzdeZyN5f/H8dc9ZzYzzNgHY8aMJVt2JaRGsjOWfImyL2n5qSQlRVKUUihKhAhRWca+ZZAlS6PIFsY+dmYss55z//44GU22OZqZM8v7+Xicx5z7uu/7Op/D9ytv13Vfl2GapomIiIiIiIiIOJ2LswsQERERERERETuFdBEREREREZFMQiFdREREREREJJNQSBcRERERERHJJBTSRURERERERDIJhXQRERERERGRTEIhXURERERERCSTUEgXERERERERySQU0kVEREREREQyCYV0ERGRTGbatGkYhpH8cnV1pXjx4nTv3p2TJ0+m++cHBQXRrVu35OPw8HAMwyA8PNyhfjZt2sS7777L5cuXbzkXEhJCSEjIf6pTREQkO3J1dgEiIiJye1OnTqVcuXLExsayfv16Ro4cybp169i1axfe3t4ZVkf16tXZvHkzFSpUcOi+TZs2MWzYMLp160bevHlTnJswYUIaVigiIpJ9KKSLiIhkUg8++CA1a9YEoH79+litVoYPH86CBQt45plnbrn++vXreHl5pXkdPj4+PPLII2nap6OBX0REJKfQdHcREZEs4kZQPnr0KN26dSN37tzs2rWLRo0akSdPHho0aABAQkIC77//PuXKlcPDw4NChQrRvXt3zp07l6K/xMREBg4cSJEiRfDy8uLRRx9l69att3zunaa7//rrr7Rs2ZICBQrg6elJqVKleOWVVwB49913ef311wEIDg5Onrp/o4/bTXe/ePEiL7zwAv7+/ri7u1OyZEkGDx5MfHx8iusMw+Cll15ixowZlC9fHi8vL6pUqcLixYvv55dVREQkU9FIuoiISBZx8OBBAAoVKsSBAwdISEggNDSU5557jjfffJOkpCRsNhutWrViw4YNDBw4kDp16nD06FGGDh1KSEgI27dvJ1euXAD07t2b6dOnM2DAABo2bMju3btp27YtV65cuWctK1asoGXLlpQvX55PP/2UwMBAjhw5wsqVKwHo1asXFy9e5PPPP2fevHkULVoUuPMIelxcHPXr1+fQoUMMGzaMypUrs2HDBkaOHMnOnTtZsmRJiuuXLFnCtm3beO+998idOzejRo2iTZs27N+/n5IlS973r7GIiIizKaSLiIhkUlarlaSkJOLi4li3bh3vv/8+efLkITQ0lI0bN5KYmMiQIUPo3r178j3ff/89y5cv56effqJt27bJ7VWqVOGhhx5i2rRpPP/88+zbt49vv/2WV199lVGjRgHQsGFD/Pz8bjuV/t9efPFFAgMD+fXXX/H09Exuv1FL8eLFCQwMBKBatWoEBQXdtb9vv/2WP/74g7lz5/K///0vuZ7cuXPzxhtvsGrVKho2bJh8fWxsLKtXryZPnjyA/bn5YsWKMXfuXN5888171i8iIpJZabq7iIhIJvXII4/g5uZGnjx5aNGiBUWKFGHZsmX4+fklX/PUU0+luGfx4sXkzZuXli1bkpSUlPyqWrUqRYoUSZ5uvnbtWoBbAnn79u1xdb37v+EfOHCAQ4cO0bNnzxQB/b/4+eef8fb2pl27dinab6wyv2bNmhTt9evXTw7oAH5+fhQuXJijR4+mST0iIiLOopF0ERGRTGr69OmUL18eV1dX/Pz8kqeM3+Dl5YWPj0+KtjNnznD58mXc3d1v2+f58+cBuHDhAgBFihRJcd7V1ZUCBQrcta4bz7YXL1489V/mHi5cuECRIkUwDCNFe+HChXF1dU2u94bb1ejh4UFsbGya1SQiIuIMCukiIiKZVPny5ZNXd7+dfwdagIIFC1KgQAGWL19+23tujD7fCLmnT5/G398/+XxSUtItgfjfChUqBMCJEyfu/gUcUKBAAX799VdM00zxvc6ePUtSUhIFCxZMs88SERHJzDTdXUREJBtp0aIFFy5cwGq1UrNmzVteZcuWBUheWX3mzJkp7p87dy5JSUl3/YwHHniAUqVKMWXKlFtWXv8nDw8PgFSNbjdo0ICrV6+yYMGCFO3Tp09PPi8iIpITaCRdREQkG3n66aeZOXMmzZo14+WXX+bhhx/Gzc2NEydOsHbtWlq1akWbNm0oX748zz77LGPGjMHNzY0nn3yS3bt388knn9wyhf52xo8fT8uWLXnkkUd49dVXCQwM5NixY6xYsSI5+FeqVAmAsWPH0rVrV9zc3ChbtmyKZ8lv6NKlC+PHj6dr164cOXKESpUq8csvvzBixAiaNWvGk08+mba/UCIiIpmUQrqIiEg2YrFYCAsLY+zYscyYMYORI0fi6upK8eLFefzxx5ODM8A333yDn58f06ZNY9y4cVStWpWffvqJp59++p6f07hxY9avX897771Hv379iIuLo3jx4oSGhiZfExISwqBBg/j222+ZNGkSNpuNtWvX3rI/OoCnpydr165l8ODBfPzxx5w7dw5/f38GDBjA0KFD0+TXRkREJCswTNM0nV2EiIiIiIiIiOiZdBEREREREZFMQyFdREREREREJJNQSBcRERERERHJJBTSRURERERERDIJhXQRERERERGRTEIhXURERERERCSTyHH7pNtsNk6dOkWePHkwDMPZ5YiIiIiIiEg2Z5omV65coVixYri43H2sPMeF9FOnThEQEODsMkRERERERCSHOX78OMWLF7/rNTkupOfJkwew/+L4+Pg4uRoRERERERHJ7mJiYggICEjOo3eT40L6jSnuPj4+CukiIiIiIiKSYVLzyLUWjhMRERERERHJJBTSRURERERERDIJhXQRERERERGRTCLHPZOeGqZpkpSUhNVqdXYpkkbc3NywWCzOLkNEREREROSuFNL/JSEhgaioKK5fv+7sUiQNGYZB8eLFyZ07t7NLERERERERuSOF9H+w2WxERkZisVgoVqwY7u7uqVp9TzI30zQ5d+4cJ06coEyZMhpRFxERERGRTEsh/R8SEhKw2WwEBATg5eXl7HIkDRUqVIgjR46QmJiokC4iIiIiIpmWFo67DRcX/bJkN5oRISIiIiIiWYHSqIiIiIiIiEgmoZAuIiIiIiIikkkopEuWEB4ejmEYXL582dmliIiIiIiIpBuF9GzAMIy7vrp165bhNR05cgTDMNi5c2ea9FenTh2ioqLw9fVNk/5EREREREQyI63ung1ERUUlv58zZw5Dhgxh//79yW25cuVKcX1iYiJubm4ZVt/dJCQk4O7ufs/r3N3dKVKkSAZUJCIiIiIi4jwaSb8X04Rr1+7vNXcuvPSS/ef93G+aqSqxSJEiyS9fX18Mw0g+jouLI2/evMydO5eQkBA8PT357rvvePfdd6latWqKfsaMGUNQUFCKtqlTp1K+fHk8PT0pV64cEyZMSFVNwcHBAFSrVg3DMAgJCQGgW7dutG7dmpEjR1KsWDEeeOABAL777jtq1qxJnjx5KFKkCJ06deLs2bPJ/f17uvu0adPImzcvK1asoHz58uTOnZsmTZqk+AcLERERERGRrMapIX39+vW0bNmSYsWKYRgGCxYsuOc969ato0aNGnh6elKyZEm++uqr9C3y+nXInfv+Xh06wPjx9p/3c//162n2Nd544w369evH3r17ady4carumTRpEoMHD+aDDz5g7969jBgxgnfeeYdvv/32nvdu3boVgNWrVxMVFcW8efOSz61Zs4a9e/eyatUqFi9eDNhH1IcPH87vv//OggULiIyMvOc0/evXr/PJJ58wY8YM1q9fz7FjxxgwYECqvpuIiIiIiEhm5NTp7teuXaNKlSp0796dp5566p7XR0ZG0qxZM3r37s13333Hxo0beeGFFyhUqFCq7s/JXnnlFdq2bevQPcOHD2f06NHJ9wUHB7Nnzx4mTpxI165d73pvoUKFAChQoMAt09S9vb2ZPHlyimnuPXr0SH5fsmRJxo0bx8MPP8zVq1fJnTv3bT8jMTGRr776ilKlSgHw0ksv8d577zn0HUVERERERDITp4b0pk2b0rRp01Rf/9VXXxEYGMiYMWMAKF++PNu3b+eTTz5Jv5Du5QVXrzp+35Il9hF0iwWsVpgzB5o3d/yz00jNmjUduv7cuXMcP36cnj170rt37+T2pKSk/7x4W6VKlW55Dj0iIoJ3332XnTt3cvHiRWw2GwDHjh2jQoUKt+3Hy8srOaADFC1aNMUUeRERERERyd6+/NIetfr3h9BQZ1eTNrLUwnGbN2+mUaNGKdoaN27MN998c8fF0OLj44mPj08+jomJcexDDQO8vR0vtn178PSE8HAICXH6/2K8//UdXFxcMP/1zHtiYmLy+xshedKkSdSqVSvFdRaLJU1ruXbtGo0aNaJRo0Z89913FCpUiGPHjtG4cWMSEhLu2M+/f78Nw7jlO4mIiIiISPbz++/w4ouwcaP9eN06WLjQ6bErTWSpkH769Gn8/PxStPn5+ZGUlMT58+cpWrToLfeMHDmSYcOGZVSJKYWGZtr/lRQqVIjTp09jmiaGYQCk2C7Nz88Pf39/Dh8+zDPPPONw/zdGyq1W6z2v3bdvH+fPn+fDDz8kICAAgO3btzv8mSIiIiIikr1t2gQjRtgnLv+TxWIfH82k8cshWW519xuB8oYbI6f/br9h0KBBREdHJ7+OHz+e7jVmBSEhIZw7d45Ro0Zx6NAhxo8fz7Jly1Jc8+677zJy5EjGjh3LgQMH2LVrF1OnTuXTTz+9Z/+FCxcmV65cLF++nDNnzhAdHX3HawMDA3F3d+fzzz/n8OHDhIWFMXz48P/8HUVEREREJOszTVi50j5BuW5de0B3cYF69YCyYRhNXsVaOoy/N5TK8rJUSC9SpAinT59O0Xb27FlcXV0pUKDAbe/x8PDAx8cnxUvsz/NPmDCB8ePHU6VKFbZu3XrLyui9evVi8uTJTJs2jUqVKvH4448zbdq05O3V7sbV1ZVx48YxceJEihUrRqtWre54baFChZg2bRo//PADFSpU4MMPP+STTz75z99RRERERESyLpsN5s2Dhx6Cxo3tU9rd3KBz78v8uGkHtd5+HTq2wqw1Fjq2grJhzi45TRhmJnmI1zAM5s+fT+vWre94zRtvvMGiRYvYs2dPctvzzz/Pzp072bx5c6o+JyYmBl9fX6Kjo28J7HFxcURGRhIcHIynp+d9fQ/JnPR7KyIiIiKSeYXtD2Nt5FrqB9enSXBLJs48w+iphzgacxDyH8JS8BAFyx4kwfsQl+Iv3HK/xbDQr1Y/Pm1871m/znC3HPpvTn0m/erVqxw8eDD5ODIykp07d5I/f34CAwMZNGgQJ0+eZPr06QD07duXL774gv79+9O7d282b97MN998w+zZs531FURERERERORvYWGwdi3Ur3/v58MvxV5i/4X9zN41m3Fbx4FpMObXMZDkAa7x0ODmtVbgDMDfa4L7efuRL1c+9p3fh4GB1bQSEhSSLt8pozk1pG/fvp369esnH/fv3x+Arl27Mm3aNKKiojh27Fjy+eDgYJYuXcqrr77K+PHjKVasGOPGjdMe6U4wYsQIRowYcdtz9erVu+X5dhERERERyb5iYuDTT2HYMPsGWWPGQI0aUNjPynWPI1zz3M/VXPu45rmfa577uJprHwlu/9o+2fh7krdrPJgGeY1AKgeUolyh0pTKX4rS+UtTKl8pSuUvRW733IB9BD78SDghQSGEls0Gq8aRiaa7ZxRNd08bFy9e5OLFi7c9lytXLvz9/TO4orvT762IiIiISNo5cwY2bLj52rkrATP/fqjyLZTYADYLeFyBAn/ZQ/edxPjD9fxQZBfYXMDFRpn9E/j1qx7k8/HIuC+UzrLMdHfJuvLnz0/+/PmdXYaIiIiIiKQz04TDh+1hfN0GK2t3Hubo9d1Q+O9X7d3Q4gBYkm57vyseFHF7gCJu5SjiWpaif/8s4laWXC55iIiAz2eHQXA4RIbwyahQ8uXg9b4V0kVERERERLIh04S//oKvvoI//oCyZaFKldTea/Ldn1OIuL6YxCu+JCSY9kBedA8Ext32HneLOwnWBAAMXGhVNpRPG39KoG8gFhfLHT+re3d4MiyU8PBQQp7PHnud/xcK6SIiIiIiItmA1Qq//35zCvovv9inpd+wZs0dbjSsUOAAFI2AIhH2n/5bocAVuM1O1x4uuShfsAJVij7Ig4Vvvnac2kHrOa2xGBasppXu1boTnO/e2zeDPZjn9HB+g0K6iIiIiIhIFhQXB9u23QzlGzfClSspr7FY7OEd7Au6BZWOI7DmLqJz7STaK4LLuSKI8fwDq+X6rR9gAgZgGhRJqs2Xz77Og4UfJDhv8G1Hxov7FGfh0wuz3UJuGU0hXUREREREJAP99BOsXAl160Ljxqm/b9kyWLgQXF3h9GnYuhUSElJe4+Nj77dePXi4Tixbjkbw9o9TIHAjptt1juY9SSTWW/r2dvOmSpEqVCtSjWpFqrHxt2imnnzNvgCci5UeZd+gdbl7h+7QsqEK5/+RQrqIiIiIiEgGuHABXnwR5syxH3/99X/v08/PHsjrPmqleNV9ROfeyvaorfxw8leGrN9Fki0Jaty83gYU9CqYHMarFbX/LJ2/dIrR8Z7VoeiM0izfG06T8iF80FnBO6MopIuIiIiIiKSjU6dg9GiYOBGuXbv1vGHcu49/bpxtGPBos5N06L+VY9Zf2XZqK0NObedK+JVb7vNy9SI2KRYTExfDhR7VevB1i68xUvGhH3QO5QMUzjOaQno2cK//g3Xt2pVp06ZlTDF/O3LkCMHBwURERFC1atU069cwDObPn0/r1q3TrE8RERERkfRw+DCMGgVTp96cll6ypL39xrPiCxfee8G0+KR4xszdzps/fG2ftu55iQ1eF9mwIeV13m7e1CxWk4f9H05+RURFpFjMreUDLVMV0MV5FNKzgaioqOT3c+bMYciQIezfvz+5LVeuXCmuT0xMxM3NLcPqExERERHJSXbvhg8/hNmzwWazt9WrB2+9ZX8GfdEiCA+HkJDbB/TLcZfZdHwTvxz7hQ3HNrDt5DbirfFQ9eY1LrhQya8StfxrJQfyCoUq3LKgW6BvoBZzy2IU0u/BNOH6bRY6TI0lS2D9enjsMWje3PH7vbxSN/WlSJEiye99fX0xDCO57ciRIxQtWpQ5c+YwYcIEtmzZwpdffsnRo0dZsGABO3fuTL53zJgxjBkzhiNHjiS3TZ06lVGjRhEZGUlQUBD9+vXjhRdeuGdNwcH2rRaqVasGwOOPP054ePg9+0xISKB///789NNPXLp0iSJFivDcc88xaNAggoKCAGjTpg0AJUqUSFGriIiIiIgzbd0KI0fCggU325o0sYfzevVutv17u7Hj0cf55dgvyaF899ndmPxjfjuQyzUXcUlxydPWX6j5Ap83+zxVdWkxt6xFIf0erl+H3Ln/Wx/jx9/ffVevgrf3f/vsG9544w1Gjx7N1KlT8fDw4OtUrFIxadIkhg4dyhdffEG1atWIiIigd+/eeHt707Vr17veu3XrVh5++GFWr15NxYoVcXd3T1Wf48aNIywsjLlz5xIYGMjx48c5fvw4ANu2baNw4cJMnTqVJk2aYLHcuu2DiIiIiEhGMk37qPiIEbB6tb3NMOCpp2DQIKhe/d/Xm4zfNp75++ZjtVk5cvkIR6OP3tJvmfxlqBdYj0cDH+XRwEfZc25PimnrDUs1TP8vJ06hkJ5DvPLKK7Rt29ahe4YPH87o0aOT7wsODmbPnj1MnDjxniG9UKFCABQoUCDFSP+9+jx27BhlypTh0UcfxTAMSpQocUufefPmTdGniIiIiMh/MW4czJ0LwcFQvnzq79uzB9assW+HBvat0Z59Ft54A8qVu3nd0ctHWRO5hjWRa1j611Iux11O0Y/FsFCtaLXkUF43oC5+uf1SXFOmQBlNW88hFNLvwcvLPqLtqCVLoEOHmwtCzJnj+JR3Ly/HP/dOatas6dD1586d4/jx4/Ts2ZPevXsntyclJeHr63tfNaSmz27dutGwYUPKli1LkyZNaNGiBY0aNbqvzxMRERERuRurFbp3hxkz7McbN95/X82b22fQligB566dY+6fa1lz2B7MD106dNt7DAzalm/LtNbTyO1+7+m7mraeMyik34Nh3N+U8/btwdPz7gtCZCTvf30JFxcXTDPlcy6JiYnJ721/r3AxadIkatWqleK6+51mnpo+q1evTmRkJMuWLWP16tW0b9+eJ598kh9//PG+PlNERERE5HZOnIAuXWDt2ptthgEVK8Ijj9z7/i1bYHdiGAStxTj1CK4VcjN23xrWLFvDH2f+SHGtxbDwsP/DNAhugKerJ2+vfTt52nqXKl1SFdAl51BIT0f/XhAiMylUqBCnT5/GNM3kLRj+uYicn58f/v7+HD58mGeeecbh/m88g261Wh3u08fHhw4dOtChQwfatWtHkyZNuHjxIvnz58fNzS1FnyIiIiIijpo3D3r1gkuXwMMD4uNvzoD94IN7/x3eZtroOflTdp96HUwwDVgIsOXmNZUKV6JBcAMalGzAYyUew8fD5+Y5v0qati53pJCeQ4WEhHDu3DlGjRpFu3btWL58OcuWLcPH5+YfHu+++y79+vXDx8eHpk2bEh8fz/bt27l06RL9+/e/a/+FCxcmV65cLF++nOLFi+Pp6Ymvr+89+/zss88oWrQoVatWxcXFhR9++IEiRYqQN29eAIKCglizZg1169bFw8ODfPnypecvk4iIiIhkI9euwSuvwOTJ9uOaNWHWLNi7994zYC9cv8DKQytZenApKw6u4Nz1c/YTf+/G5Ovhy/8q/I8GJRvwRPATFPYufMc6NG1d7kYhPYcqX748EyZMYMSIEQwfPpynnnqKAQMGpFj1vVevXnh5efHxxx8zcOBAvL29qVSpEq+88so9+3d1dWXcuHG89957DBkyhHr16hEeHn7PPnPnzs1HH33EX3/9hcVi4aGHHmLp0qW4uLgAMHr0aPr378+kSZPw9/fXFmwiIiIikiq//QYdO8KBA/Zp7W+8AcOGgbs7lClzazi3mTZ2nt7J0r+WsvSvpfx68ldspi35fC7XXMQmxeJiuGAzbUxvM13BW9KEYf77weRsLiYmBl9fX6Kjo1OMGgPExcURGRlJcHAwnp6eTqpQ0oN+b0VERERyJpsNRo+GwYMhMRH8/e0LxdWvf/OasP1hrI1cy0P+D+Hm4sbSg0tZfnA5p6+eTtHXg4UfpFnpZjQr04w6AXVYdnCZpq1Lqtwth/6bRtJFRERERCRbOnkSuna1b5MG0LYtfP01FChw85qJ2yfSd0nf297v7ebNkyWfpFmZZjQt3ZQA34AU5zVtXdKDQrrclxEjRjBixIjbnqtXrx7Lli3L4IpERERERG5asAB69oSLF+1bG48daz8Gk4ionczfN5/5++az++zuFPfl88xH96rdaVamGY8GPoqHq4czypccTCFd7kvfvn1p3779bc/lypUrg6sRERERkewmLMy+PVr9+qnfMSksDFauhIMHYcUKe1v16jDjOyvnvTbSf8V8FuxfwJHLR5LvccEFG7bkZ8untZ6m0XFxKoV0uS/58+cnf/78zi5DRERERLKZkyfhs8/sz5EbBowZYw/ppUrd/b5Dh+whPZlrHG0HrMG31nxCFofdXI0d+6JvjUs3pk25NrR4oAW/HPtFz5ZLpqGQLiIiIiIiTmGa8NdfsGGD/bV+PURGpjwP/wrfd1NxDlT9FtxjcC3+O/MsV+F3+6m8nnlp+UBL2pRrQ+PSjfFy80q+Tc+WS2aikC4iIiIiIhnCaoXff78Zyn/5Bc6cSXmNiwsEBcHhw/aRdNOEp56yb5N2O0nEcchlGesSRnMx98Z/tEOxPMVoXbY1bcq34fESj+NmcUu37yaSVhTSRUREREQkXZgmjBplHwmPjbU/K37lSsprPDzg4YehXj37q04d8PGx3xMeDiEhtz6TnmRLYm3kWmbvns28vfOIjo8G95vnDQw6VOzAzKdm4mK4pPfXFElTCukiIiIiIpKmrFaYPx/eeMM+Iv5PPj5Qt+7NUP7QQ/ag/m+hoSnDuWmabD6xmdm7ZjN3z1zOXjubfM4/jz81i9Zk4YGFWAwLVtNKx0odFdAlS1JIFxERERGRNJGYCDNnwocfwv79Kc+5uEDHjvDtt2CxpK4/0zTZdXYXs3fNZvbu2RyNPpp8rkCuArSr0I6OD3akXol6uBguhO0P0wJwkuUppItD3n33XRYsWMDOnTsB6NatG5cvX2bBggUZWseRI0cIDg4mIiKCqlWrZuhni4iIiEhKsbHwzTfw8cdw7Ji9LW9eaNwY5syxh3KrFdq3T11AP3r5KG/9/BbLDy7nYuzF5Pbc7rlpXa41HR/sSMOSDW95xlwLwEl2oJCeTXTr1o1vv/0WAFdXVwICAmjbti3Dhg3D29s73T537NixmDeW3bwHBWsRERGR7CUmBiZMsG+Zdvbv2ed+fvDaa/Dcc/ap7Z063fnZ8n+KTYxlwb4FTNk5hdWHV6c494j/I/Sv3Z/mDzRPsSq7SHakkJ6NNGnShKlTp5KYmMiGDRvo1asX165d48svv0xxXWJiIm5uabOypa+vb5r0IyIiIiJZx/nzMHYsfP45REfb20qUgIEDoXt3yJXr5rX/frb8n0zTZEfUDqZETGH27tlcjrt8yzUWw0LtgNr8r+L/0v6LiGRCWkkhG/Hw8KBIkSIEBATQqVMnnnnmGRYsWMC7775L1apVmTJlCiVLlsTDwwPTNImOjqZPnz4ULlwYHx8fnnjiCX7//fcUfX744Yf4+fmRJ08eevbsSVxcXIrz3bp1o3Xr1snHNpuNjz76iNKlS+Ph4UFgYCAffPABAMHBwQBUq1YNwzAICQlJvm/q1KmUL18eT09PypUrx4QJE1J8ztatW6lWrRqenp7UrFmTiIiINPyVExEREZF7CQuDXr3sgbtECXj/fXtAL1fO/pz5X3/BCy+kDOh3cu7aOcZsGUOVr6rw0KSH+HL7l1yOu0ygbyBDHx/K1y2+BkheBC4kKCR9v5xIJqKR9HswTZPridfv694lfy1h/dH1PFbiMZqXae7w/V5uXhiGcV+fDZArVy4SExMBOHjwIHPnzuWnn37C8veDQM2bNyd//vwsXboUX19fJk6cSIMGDThw4AD58+dn7ty5DB06lPHjx1OvXj1mzJjBuHHjKFmy5B0/c9CgQUyaNInPPvuMRx99lKioKPbt2wfYg/bDDz/M6tWrqVixIu7u9n0yJk2axNChQ/niiy+oVq0aERER9O7dG29vb7p27cq1a9do0aIFTzzxBN999x2RkZG8/PLL9/3rIiIiIiKOmTQJ+vRJ2Va9OgweDK1b2xeFu5ckWxIrDq5gys4pLNq/iESb/e+pHhYPnqrwFD2q9qB+cP3kFdn9cvtpETjJkRTS7+F64nVyj8z9n/oYv238fd13ddBVvN3v73nyrVu3MmvWLBo0aABAQkICM2bMoFChQgD8/PPP7Nq1i7Nnz+Lx954Xn3zyCQsWLODHH3+kT58+jBkzhh49etCrVy8A3n//fVavXn3LaPoNV65cYezYsXzxxRd07doVgFKlSvHoo48CJH92gQIFKFKkSPJ9w4cPZ/To0bRt2xawj7jv2bOHiRMn0rVrV2bOnInVamXKlCl4eXlRsWJFTpw4wfPPP39fvzYiIiIi4pjFi1Met2kDP/0EqRlP+nL7l0z5bQqHLh3iUtyl5PaHij1E96rdefrBp8mXK98t92kROMmpFNKzkcWLF5M7d26SkpJITEykVatWfP7550yYMIESJUokh2SAHTt2cPXqVQoUKJCij9jYWA4dOgTA3r176du3b4rztWvXZu3atbf9/L179xIfH5/8DwOpce7cOY4fP07Pnj3p3bt3cntSUlLy8+579+6lSpUqeHndXCSkdu3aqf4MEREREflvune3T3d3cQGbDbp1u3tAT7IlsWj/IoaGD2XX2V3J7Xnc89Crei+6V+1OJb9K6V+4SBakkH4PXm5eXB101eH7lvy1hA4/dkh+jmZOuzkOT3l3dOXK+vXr8+WXX+Lm5kaxYsVSLA737xXebTYbRYsWJTw8/JZ+8ubN69Dn3pArNQ8g/YvNZgPsU95r1aqV4tyNafmpXT1eRERERNJH69awcOG9V2k/e+0sk3ZMYuKOiRyPOZ7inIvhQvdq3fm08afpXa5IlqaQfg+GYdzXlPP2Fdvj6eqZoc/ReHt7U7p06VRdW716dU6fPo2rqytBQUG3vaZ8+fJs2bKFLl26JLdt2bLljn2WKVOGXLlysWbNmuQp8v904xl0q9Wa3Obn54e/vz+HDx/mmWeeuW2/FSpUYMaMGcTGxib/Q8Dd6hARERGRtHenVdpN02TLiS2M3zaeH/b8QII1AYCCXgUJKRHCj3t/TB64ahCc+hmXIjmVQno6yszP0Tz55JPUrl2b1q1b89FHH1G2bFlOnTrF0qVLad26NTVr1uTll1+ma9eu1KxZk0cffZSZM2fy559/3nHhOE9PT9544w0GDhyIu7s7devW5dy5c/z555/07NmTwoULkytXLpYvX07x4sXx9PTE19eXd999l379+uHj40PTpk2Jj49n+/btXLp0if79+9OpUycGDx5Mz549efvttzly5AiffPJJBv+KiYiIiMg/xSbGMnv3bMZvG89vUb8lt9fyr8WLD73I/yr+D09XT8L2h2kBOBEHKKTnUIZhsHTpUgYPHkyPHj04d+4cRYoU4bHHHsPPzw+ADh06cOjQId544w3i4uJ46qmneP7551mxYsUd+33nnXdwdXVlyJAhnDp1iqJFiyY/1+7q6sq4ceN47733GDJkCPXq1SM8PJxevXrh5eXFxx9/zMCBA/H29qZSpUq88sorAOTOnZtFixbRt29fqlWrRoUKFfjoo4946qmn0v3XSURERERSOnzpMF9u+5IpO6dwMfYiYF+hvWOljrz40IvULFYzxfWZeeBKJDMyzBz2wG9MTAy+vr5ER0fj4+OT4lxcXByRkZEEBwfj6enppAolPej3VkREROT+mabJe+ve49vfvyXycmRye1DeIJ6v+Tw9qvWgoFdBJ1YokrndLYf+m0bSRURERETkjn4//Tud53dOsUp7tSLVGBYyjGZlmmFxsTixOpHsx8XZBYiIiIiISAYKC4OXX7b/vIszV8/QO6w31SZWSxHQLYaFkKAQWpZtqYAukg4U0kVEREREcorvv4dWreDzz+0/bxPU45Li+PCXDynzeRkmR0zGxKRuQF2A5FXaQ4JCMrhwkZxD091FRERERHKCLVvguefs700TLBb7xud/76tmmiY/7vmRgasHcuTyEQBqFqvJZ40/49HAR7VKu0gGUUi/jRy2ll6OoN9TERERybGsVhgxAoYNs78HcHGxvw8JAWD7qe28uuJVfjn2CwD+efwZ2WAkz1R+BhfDPvlWq7SLZAyF9H9wc3MD4Pr16+TKlcvJ1UhaSkhIAMBi0XNTIiIikoMcPQrPPgu/2MM3HTtCixawfTuEhHAypAZvLejK9N+nA5DLNRcD6w7k9Tqv4+3u7cTCRXIuhfR/sFgs5M2bl7NnzwLg5eWFYRhOrkr+K5vNxrlz5/Dy8sLVVf+TFxERkRxizhz79PboaMidGyZMsAd2w+D6/1rzyaZP+OiLjlxPvA5A58qdGdFgBMV9iju5cJGcTYnlX4oUKQKQHNQle3BxcSEwMFD/6CIiIiLZ35Ur8H//B99+az+uVQtmzoRSpQAYuGogX27/kqsJVwGoE1CHzxp/xsP+DzurYhH5B4X0fzEMg6JFi1K4cGESExOdXY6kEXd3d1xctJmBiIiIZHNbt0KnTnDokP2587fegiFD4O/HOjvP68x3u75LvnxA7QGMajhKAxkimYhC+h1YLBY9vywiIiIiWYPVCh99BEOHQlISBATYR8/r1QPsi+i+teatFAH9xnZqCugimYtCuoiIiIhIVnb8OHTuDOvW2Y/bt4evvoJ8+QCwmTb+b+n/MWH7hORbtN+5SOalkC4iIiIikhWFhcHEifZwfu0aeHvDF19A167w9+h4ki2JnmE9mf77dAwMvmrxFUVyF9F+5yKZmEK6iIiIiEhWExYGrVrdPC5dGpYts//8W3xSPJ3mdWLe3nlYDAvT20ynU6VOAArnIpmYQrqIiIiISFYzb97N94YBzZunCOjXE6/Tdk5bVhxagbvFnbnt5tKqXKvbdCQimY1CuoiIiIhIVnPhgv2nYYBpwhNPJJ+KiY+hxawWbDi2AS83LxZ0WEDDUg2dVKiIOEp7UomIiIiIZCVXr8L69fb3rVvDwoUQap++fuH6BRpMb8CGYxvw8fBh5bMrFdBFshiNpIuIiIiIZCXffQcxMfbp7T/+aN8PHYi6EkXDGQ3589yfFPQqyIpnV1C9aHUnFysijlJIFxERERHJKkwTxo+3v3/xxeSAfvTyURpMb8ChS4colqcYqzuvpnyh8k4sVETul0K6iIiIiEhWsWED7N4NXl7QrRsABy4coMH0BpyIOUFw3mBWd1lNyXwlnVuniNw3hXQRERERkazixij6M89A3rz8ceYPGs5oyNlrZylXsByrO6/G38ffuTWKyH+iheNERERERLKCU6dubr324ot8vPFjHp70MGevnaVakWqs77ZeAV0kG9BIuoiIiIhIVvD115CUBI8+ypjraxm4emDyqQF1BlDIu5ATixORtKKRdBERERGRzC4hASZOBOBA77a89fNbyacshoXtp7Y7qzIRSWMK6SIiIiIimd38+XD6NCdLFqTRxbHEJsUC9oBuNa2EBIU4tz4RSTOa7i4iIiIiktmNH8+FXNCos8HR6KOUyV+Gtx97m52ndxISFEJo2VBnVygiaUQhXUREREQkM9u1i6u/bqB5F9hjnMM/jz+rOq+iRN4SdKnSxdnViUga03R3EREREZFMLH78ONp2gF+LQ/5c+VnZeSUl8pZwdlkikk4U0kVEREREMinrxQt0jp7KqlLg7eLJ0k5LqVCogrPLEpF0pJAuIiIiIpIJmabJCxNb8kM5K242g/kdF1KreC1nlyUi6UwhXUREREQkE3p7zWC+TtiMYcKsvD1pWLqRs0sSkQygheNERERERDKZTzd/yoiNIwGYuNqTdss/c3JFIpJRNJIuIiIiIpKJTNs5jddWvgbAyNXQu3pvyJ3byVWJSEbRSLqIiIiISCaxcN9CeoX1AuC1zfDGL8CkF5xblIhkKI2ki4iIiIhkAuFHwunwYwesppVuSZX4eAUYDRpAuXLOLk1EMpDTQ/qECRMIDg7G09OTGjVqsGHDhrteP3PmTKpUqYKXlxdFixale/fuXLhwIYOqFRERERFJe79F/Ubo7FDirfG0KtOCSV+dxAB46SVnlyYiGcypIX3OnDm88sorDB48mIiICOrVq0fTpk05duzYba//5Zdf6NKlCz179uTPP//khx9+YNu2bfTq1SuDKxcRERERSRtfbv+SR6c8ypWEKzxe4nG+T2iF6/mLEBAALVo4uzwRyWBODemffvopPXv2pFevXpQvX54xY8YQEBDAl19+edvrt2zZQlBQEP369SM4OJhHH32U5557ju3bt2dw5SIiIiIi/920ndN4YckLxCbFAtC3Zl88v5xkP9m3L7hqCSmRnMZpIT0hIYEdO3bQqFHK/R4bNWrEpk2bbntPnTp1OHHiBEuXLsU0Tc6cOcOPP/5I8+bN7/g58fHxxMTEpHiJiIiIiDjb5bjLDFg5IPnYYljYuiMMtm4Fd3fQbFGRHMlpIf38+fNYrVb8/PxStPv5+XH69Onb3lOnTh1mzpxJhw4dcHd3p0iRIuTNm5fPP//8jp8zcuRIfH19k18BAQFp+j1ERERERBwVlxRH6+9bcyHWvraSxbBgNa2E/Pr334Pbt4fChZ1YoYg4i9MXjjMMI8WxaZq3tN2wZ88e+vXrx5AhQ9ixYwfLly8nMjKSvn373rH/QYMGER0dnfw6fvx4mtYvIiIiIuIIq81K5/mdWXd0HXnc8/BZo8/oV6sfC5vNIHTK3zNKX3zRuUWKiNM47SGXggULYrFYbhk1P3v27C2j6zeMHDmSunXr8vrrrwNQuXJlvL29qVevHu+//z5Fixa95R4PDw88PDzS/guIiIiIiDjINE1eXv4yP+75EXeLOwueXsATwU/YT44aBfHxUL061Krl3EJFxGmcNpLu7u5OjRo1WLVqVYr2VatWUadOndvec/36dVxcUpZssVgA+x94IiIiIiKZ2chfRjJ+23gMDGa0mXEzoFutcGPx5JdegjvMLBWR7M+p09379+/P5MmTmTJlCnv37uXVV1/l2LFjydPXBw0aRJcuXZKvb9myJfPmzePLL7/k8OHDbNy4kX79+vHwww9TrFgxZ30NEREREZF7mhIxhcE/DwZgTJMxtK/Y/ubJpUvhyBHInx+efto5BYpIpuDUPR06dOjAhQsXeO+994iKiuLBBx9k6dKllChRAoCoqKgUe6Z369aNK1eu8MUXX/Daa6+RN29ennjiCT766CNnfQURERERkXtafGAxfRb1AeDNum/Sr1a/lBeMH2//2aMH5MqVwdWJSGZimDlsnnhMTAy+vr5ER0fj4+Pj7HJEREREJJvbfHwzDaY3IDYplq5VujK11dSUCyV/9RU8/7z9/aFDULKkcwoVkXTjSA51+uruIiIiIiLZ1b7z+2gxuwWxSbE0K9OMSS0npQzoYWE3AzrA7t0ZX6SIZCoK6SIiIiIi6eBkzEkaf9eYi7EXedj/Yea2m4ubxS3lRVOm3Hzv4gLh4Rlao4hkPgrpIiIiIiJp7HLcZZrMbMKx6GM8UOABlnRagre7d8qLdu+GFSvs7w0DbDYICcnwWkUkc3HqwnEiIiIiItlNXFIcrb5vxe6zuymSuwgrnl1BQa+CKS86exZatIC4OHjwQWjQAJ54AkJDnVO0iGQaCukiIiIiImnEarPyzLxnWH90PT4ePix/ZjlBeYNSXhQXB23awNGjULo0rFtn33pNRARNdxcRERERSRML9y2k+sTqzNs7D3eLOws6LKBKkSopLzJN6NMHNm2CvHlh8WIFdBFJQSPpIiIiIiL/0fy982k7t23y8Su1XqF+cP1bL/zwQ5gxAywW+OEHKFs2A6sUkaxAI+kiIiIiIv9B1JUoXlr2UvKxi+FCoi3x1gvnzYO33rK///xzePLJDKpQRLIShXQRERERkfu0NnIt1SZW49SVU4A9oNtMGyFBISkvjIiAzp3t7//v/1LujS4i8g+a7i4iIiIi4iCbaeOjXz7i7bVvYzNtVParzPM1n+fAhQOEBIUQWvYfq7SfOgUtW8L169C4MXz6qfMKF5FMTyFdRERERMQBF2Mv0mV+F5b8tQSAblW7Mb7ZeLzcvG69+Pp1aNUKTp6E8uVhzhxw1V/BReTO9CeEiIiIiEgqbT+1nXZz23E0+iierp6MbzaeHtV63P5imw26d4ft26FAAVi0CHx9M7ZgEclyFNJFRERERO7BNE2+2v4Vr6x4hQRrAqXyleLH9j9StUjVO9/03nswdy64udkXjStVKsPqFZGsK01C+uXLl8mbN29adCUiIiIikqlcTbjKc4ufY9auWQC0KdeGqa2m4ut5l1Hx77+HYcPs7ydOhMcey4BKRSQ7cHh1948++og5c+YkH7dv354CBQrg7+/P77//nqbFiYiIiIg4095ze3l40sPM2jULi2Hhk4af8FP7n+4e0H/9Fbp1s79//XX7lHcRkVRyOKRPnDiRgIAAAFatWsWqVatYtmwZTZs25fXXX0/zAkVEREREnGH2rtk8NOkh9p7fS7E8xQjvFs5rdV7DMIw733T8uH2huPh4CA2FkSMzrmARyRYcnu4eFRWVHNIXL15M+/btadSoEUFBQdSqVSvNCxQRERERyUjxiXG0+bgGyxL3ANCg4MPMqjmSwhdzw8Wdd75x7VoYMQLOn4fKlWHmTLBYMqZoEck2HA7p+fLl4/jx4wQEBLB8+XLef/99wL6YhtVqTfMCRUREREQyhNXKkVkTaBjRn4O+ScnNL32+lcL7GzjWV79+kDt3GhcoIjmBwyG9bdu2dOrUiTJlynDhwgWaNm0KwM6dOyldunSaFygiIiIikq4SEmDGDJbMeIfOj0RxyRcwAQMsNlhfxpXWMYXu3kdMDFy7Zn/v4gJ//pneVYtINuVwSP/ss88ICgri+PHjjBo1itx//wthVFQUL7zwQpoXKCIiIiKSLq5dg8mTSRr9MUPLnGREfXtzmfg8/OVxBYsNrC4Q0n4gLPrg7n2FhdmfRbdYwGqFkJB0L19EsifDNE3T2UVkpJiYGHx9fYmOjsbHx8fZ5YiIiIhIRrt8GSZMgM8+40zseTq2g7XB9lP/V60vnzQfy/JZwwjfu5yQ8k0I7XyPgH5DWBiEh9sDemhoOhUvIlmRIzn0vkL6jBkzmDhxIocPH2bz5s2UKFGCMWPGEBwcTKtWre678IygkC4iIiKSQ509C2PGwPjxEBPD+hLwdAcLUV5WcrvnZnLLyXR4sIOzqxSRbMiRHOrwFmxffvkl/fv3p2nTply+fDl5sbi8efMyZsyY+ypYRERERCRdhIVBr17QsiUEBcHIkZgxMXzcxo8nursQ5WWlYqGKbOu9TQFdRDIFh59J//zzz5k0aRKtW7fmww8/TG6vWbMmAwYMSNPiRERERETu25w58PTTKZou165Gt/+5sTBmKwDPVn6Wr5p/hbe7tzMqFBG5hcMhPTIykmrVqt3S7uHhwbUbK1qKiIiIiDjb0KEpDn975gn+9/ARDl86jLvFnc+bfk7v6r0xDMNJBYqI3MrhkB4cHMzOnTspUaJEivZly5ZRoUKFNCtMREREROS+ff897N8PgOliMLmqyf89sIH4S4kE5w3mh//9QI1iNZxcpIjIrRwO6a+//jovvvgicXFxmKbJ1q1bmT17NiNHjmTy5MnpUaOIiIiISOqdOAHPPw/A3F61ebfofvZaLoKZSMsHWvJt62/Jlyufk4sUEbk9h0N69+7dSUpKYuDAgVy/fp1OnTrh7+/P2LFjefpfz/yIiIiIiGQomw26dYPLl/miXSD/V3xz8qmuVboypdUUXAyH104WEckwDoX0pKQkZs6cScuWLenduzfnz5/HZrNRuHDh9KpPRERERCT1xo3DXLOGCXXceKXSSfh7s2EXw4X8ufIroItIpudQSHd1deX5559n7969ABQsWDBdihIRERERcdju3Zx57w16dIKlDyQmB3SLYcFqWgkJCnFqeSIiqeHwdPdatWoRERFxy8JxIiIiIiJOEx/PktdC6d4rgXPe4GHx4OOGHxPoG8i6o+sICQohtGyos6sUEbknh0P6Cy+8wGuvvcaJEyeoUaMG3t4p95SsXLlymhUnIiIiInIv1xOv8/rwR5lQJxKAyvkrMLPDHB4s/CAArcq1cmZ5IiIOMUzTNB25wcXl1ud4DMPANE0Mw8BqtaZZcekhJiYGX19foqOj8fHxcXY5IiIiIvIf7Dy9k04zWrP3+lEAXi3YkhHPzcXT1dPJlYmI3ORIDnV4JD0yMvK+CxMRERERSQs208anmz/lrTVvkWhLpOgV+Da2MQ2Hhjm7NBGR/8ThkK5n0UVERETEmU7EnKDrgq78HPkzAG32wte/B1Jw6w9OrkxE5L9zOKRPnz79rue7dOly38WIiIiIiNzNj3t+pM+iPlyKu4SX4cHYhfH03GlgrJ8FefI4uzwRkf/M4WfS8+XLl+I4MTGR69ev4+7ujpeXFxcvXkzTAtOankkXERERyXrm7J7De+vfY8+5PQDULFiFmZ8e4YHD0fDWW/DBB06uUETkztL1mfRLly7d0vbXX3/x/PPP8/rrrzvanYiIiIjIbVltVrac2MKnmz9l3r55ye3tyrdj1pTLuB3+HapXh6FDnViliEjacjik306ZMmX48MMPefbZZ9m3b19adCkiIiIiOdDVhKusOrSKsANhLD6wmPPXz6c472K4EBB5AbcVa8HTE777DtzdnVStiEjaS5OQDmCxWDh16lRadSciIiIiOcSJmBMsPrCYsP1h/Bz5M/HW+ORzeT3zUtmvMuuPrsdiWLCaVkK++8V+8uOPoXx5J1UtIpI+HA7pYWEpt7UwTZOoqCi++OIL6tatm2aFiYiIiEj2cyOIB/kGER0fTdiBMH6L+i3FNSXzlST0gVBCy4byaOCjuFncCNsfRvihNYR8sZjQ3YehUSN44QUnfQsRkfTjcEhv3bp1imPDMChUqBBPPPEEo0ePTqu6RERERCQbSbAmMGLDCIatG3bLOQOD2gG1aflAS0LLhlK+YHkMw0hxTWjZUEKn/wrLDkP+/DB1Kri4ZFT5IiIZxuGQbrPZ0qMOEREREclmLsddZulfS1m4fyHL/lrGlYQrKc6XyleKwfUG0/yB5hT2Lnz3zj76CEaMsL+fOBGKFUunqkVEnCvNnkkXERERETl6+SgL9y9k4f6FrD+6niRbUvK5vJ55uRx3GRfDBZtp49PGnxJaNvTOnSUmwm+/wVdfwbRpN9u1UJyIZGOpCun9+/dPdYeffvrpfRcjIiIiIlmLaZpEnI5g4T57MP/9zO8pzlcsVJHQsqG0KtuKh/wfYvGBxYQfCSckKOTWgH7tGmzZAhs22F9btsD16ymvsVggPBxC7xLuRUSysFSF9IiIiFR19u9nh0REREQk+0mwJvDhhg+Zt28eJ2NOcj725jZpLoYLjwY+SquyrQgtG0rp/KVT3BtaNvRmOL9wAX755WYo/+03SEpKcT3580Pp0rB1qz2gW60QEpLO31BExHkM0zRNZxeRkWJiYvD19SU6OhofHx9nlyMiIiKSJVyJv8Lyg8tZsH8BC/Yu4HrSzRFud1xp7l6BVm6Vae5ekYIuue/c0e+/w6+/wuXLcPz4recDAqBevZuv8uXtC8SFhdlH0ENCNIouIlmOIzlUIV1EREREbuvM1TMsOrCI+fvms/rwahKsCTdPmoABLjZ4cSuMW36fH1K+fMpQXqJEWpQuIpKpOJJDUzXdvW3btkybNg0fHx/atm1712vnzZuX+kpFREREJFM5ePEgC/YtYMG+BWw6vgmTm+M5pXOXoM3OWApEnuXNhmCxgdUFnowvBu0fvXfnERFw8CCYpn10vHdv+6JwIiKSLFUh3dfXN/l5c19f33QtSEREREQyzsJ9C5nz5xySbEnsObeHP8/9meL8Q8UeonXZVrT+00r5AR9iXI+FPHkof/4K4cEGIZEmoaO+TN0U9LAwaNXq5rPlzZql07cSEcm6NN1dREREJIdJsiWx/uh6Pt30KUsOLklxztXFlfpB9WldrjWhZUMpnuRlH/G+MVuyQQOYPh22b7+/Z8T1bLmI5EB6Jv0uFNJFREQkJ4pNjGXloZXM3zefRQcWcTH2YorzBgZNSjdh1lOzyOuZ1964di107gwnT4KbG3zwAbz2mn2quoiIpFqaP5P+bz/++CNz587l2LFjJCQkpDj322+/3U+XIiIiIpLGLsddZvGBxczfN5/lB5dzPfHmiuwFvQpS1a8qqyNXYzEsWE0rfWv2tQf0hAQYMgRGjbI/P/7AAzB7NlSv7rwvIyKSQzgc0seNG8fgwYPp2rUrCxcupHv37hw6dIht27bx4osvpkeNIiIiInIPYfvDWBu5lipFqhCbGMv8ffNZe2QtSbab+44H+gbSplwb2pRrQ93Auri6uBK2P4zwI+GEBIXY9y//6y/o1Mk+nR3sU90/+wy8vZ30zUREchaHp7uXK1eOoUOH0rFjR/LkycPvv/9OyZIlGTJkCBcvXuSLL75Ir1rThKa7i4iISHYzcftE+i7pe9tzFQtVtAfz8m2oVqRa8mLAtzBNmDoV+vWDa9cgXz6YPBnusbOPiIjcW7pOdz927Bh16tQBIFeuXFy5cgWAzp0788gjj2T6kC4iIiKS1Zmmye6zu5m3dx7z9s3jjzN/pDhfJHcRXn3kVdqUa0OZAmXu3eGlS9CnD/z4o/24fn374nDFi6dD9SIicjcOh/QiRYpw4cIFSpQoQYkSJdiyZQtVqlQhMjKSHLYGnYiIiEiGsZk2tp3clhzMD148mHzOBRds2HAxXLCZNia2mGifun43pglHj9qnsk+ZAlevgqsrvP8+DBhg3yZNREQynMMh/YknnmDRokVUr16dnj178uqrr/Ljjz+yfft22mo6lIiIiEiaSbIlseHoBubtncf8ffM5eeVk8jkPiweNSzembbm2tHigBRuPb0z5bPm/2WywZw9s2HDzdeJEyms+/NC+eruIiDiNw8+k22w2bDYbrq72fD937lx++eUXSpcuTd++fXF3d0+XQtOKnkkXERGRzOynPT8x448ZXEu4RsTpCC7EXkg+l9s9N83LNKdt+bY0Ld2UPB557txRYiLs2HEzkG/cCBdTbruGYdhH1ME+ct6vH3z6aTp8KxGRnC1d9kkPDAwkIiKCAgUKAPDFF1/QpUuXLBd0FdJFREQks4lNjGX5weWM/XUs646uS3Euv5svrYo8Ttui9XmyYC08LR6372TdOnsYd3eHs2dhyxaIjU15jZcXPPIIPPYY1KsH589Dhw72gG61wsKFEHqPafIiIuKwdFk47sSJE1it1uTjt956i2bNminoioiIiNyHqwlXWfrXUn7c8yNL/1rKtcRrKc4bJrTdA9//FI2rLQwIc/xD8ueHRx+1B/J69ez7nLu5pbzG0xPCwyEkRAFdRCQTcPiZ9Bu0SJyIiIiIY6Ljoll8YDE/7v2R5QeXE5cUl3wu8KqFqieshJUDiw2sLtBlvweuRQveu+PLl+3bpoF9CvsTT8DYsVC+PLi43P3e0FCFcxGRTOS+Q7qIiIiI3F3Y/jCW/bUMd4s7hy4dYtXhVSRYE5LPlzIK0O63OJ7ado2ap6wYPj6ERcQQHmwQEmkSOmpu6gJ0WBi0anVz2nq/flCxYjp+MxERSS8OhfTJkyeTO3duAJKSkpg2bRoFC6b8191+/fqlXXUiIiIiWdDF2IsMXTuUL7Z9ccu5cvnK0O5SUdrN2knlAxcwwL4f+biB0LMnoatXExoeDs+HpH6EOzTU/jy5pq2LiGR5qV44LigoCMMw7t6ZYXD48OE0KSy9aOE4ERERSQ+X4y6zcN9C5vw5h1WHV5FkS0pxvrZfDSZHVqbChB/se5IDlCkDb74Jzz5rX/BNRESypXRZOO7IkSP/tS4RERGRbCUmPoaw/WHM/XMuKw6tSDGVPcg3iCPRR7CYBlbD5M0vfqfCnzvsJ6tUgbfegqeesk9RFxER+ZueSRcRERFxwNWEqyw+sJi5f85l6V9LibfGJ5+rWKgiHSp2oH2hxyk7dRFhiz4hPMgk5AiE7k+C2rVh8GBo1sy+wJuIiMi/pHq6e3ah6e4iIiLiqB/+/IFvf/+WS3GXiIiKIDbp5v7jZQuUpUPRJ2l/rjAVtxy271X+78f/DAPatYM5cxTORURyoHSZ7i4iIiKSk8QnxbPy0Eo+2fwJ64+uT3GulFdxOljL0363jcpf7sI4Mz7lzS4uEBRkD+s3Vlx/9lkFdBERuSeFdBEREck5JkyA1avtz4TXrn3L6STTytronXx/fh3zLm7ksvVqivOGCR33uvLd3BMYnLh5wsMDHn4Y6tWzv+rUAR8f+9ZoWnFdREQckKrp7v3792f48OF4e3uzfv166tSpg6tr1sz3mu4uIiKSA23YAK+8Ar/9dsspmwGbAuD7B+GHCnA2981zRa9AzZOwqBxYbGB1gYWzITTKB+rWvRnKa9YET8+M+z4iIpKlOJJDUxXS3dzcOHHiBH5+flgsFqKioihcuHCaFZyRFNJFRERyCNOEFSvggw/gl19SngJ2VPDl+2ruzPG/zAmvxORzBeIttDuVl6dP5KXeeW8sJ08RVuA84UEQctQgtEYn+PZbrcouIiKplubPpAcFBTFu3DgaNWqEaZps3ryZfPny3fbaxx57zPGKRURERNKKzQbz5sGIERARYW9zd4eQEL64tJIZVeGYD5zOE518i4+HD23KteHpB5+mQXAD3CxuN/sLCyO0VStCD/79bPlH7RXQRUQk3aRqJH3BggX07duXs2fPYhgGd7rFMAysVqtDBUyYMIGPP/6YqKgoKlasyJgxY6hXr94dr4+Pj+e9997ju+++4/Tp0xQvXpzBgwfTo0ePVH2eRtJFRESyqcREmDULPvwQ9u2zt3l5cfT5Tsx+oiBfHZjN0eijyZe7u7jTprw9mDcp3QRP17tMV9ez5SIi8h+k+XT3G65evYqPjw/79++/43R3X1/fVBc6Z84cOnfuzIQJE6hbty4TJ05k8uTJ7Nmzh8DAwNve06pVK86cOcP7779P6dKlOXv2LElJSdSpUydVn6mQLiIiks3ExsKUKTBqFBw7BsC5oj780Lsus/wvsDFq6y23uBguPF/zeb5o9kVGVysiIjlQuoV0gHXr1lG3bt00WTiuVq1aVK9enS+//DK5rXz58rRu3ZqRI0fecv3y5ct5+umnOXz4MPnz57+vz1RIFxERyQbCwuzPm1+9CsuXw9mzXHGHhQ/7MKtRUVaaB7Ga9tl9BgYhQSFUKFSB8dvGYzEsWE0rC59eSGhZjYqLiEj6S9d90h9//HGsVis//fQTe/fuxTAMypcvT6tWrbA48HxWQkICO3bs4M0330zR3qhRIzZt2nTbe8LCwqhZsyajRo1ixowZeHt7ExoayvDhw8mVK9dt74mPjyc+Pj75OCYmJtU1ioiISCa0cCG0bg1AggWWl4ZZTbwIK5lILDFgs/+3vkbRGnSq1IkOFTvg7+MPQKNSjQg/Ek5IUIgCuoiIZEoOh/SDBw/SvHlzTpw4QdmyZTFNkwMHDhAQEMCSJUsoVapUqvo5f/48VqsVPz+/FO1+fn6cPn36tvccPnyYX375BU9PT+bPn8/58+d54YUXuHjxIlOmTLntPSNHjmTYsGGOfUkRERHJnGw2bEOHMLKefcu0yLxwzQPgOgBl8pehU6VOdHywI2ULlr3l9tCyoQrnIiKSqTkc0vv160fJkiXZvHlz8pTzCxcu8Oyzz9KvXz+WLFniUH+GYaQ4Nk3zlrYbbDYbhmEwc+bM5GffP/30U9q1a8f48eNvO5o+aNAg+vfvn3wcExNDQECAQzWKiIiIc5mmye+nfmPm6K5MafwnF71unsvn4k33h5+jY6WO1Cha445/jxAREckKHA7p69atY8uWLSmeCS9QoAAffvghdevWTXU/BQsWxGKx3DJqfvbs2VtG128oWrQo/v7+KRanK1++PKZpcuLECcqUKXPLPR4eHnh4eKS6LhEREck8Dl86zOxds5n5x3fsvbAPbvwVwAQMcMGgy0O9GN14tDPLFBERSTMujt7g4eHBlStXbmm/evUq7u7uqe7H3d2dGjVqsGrVqhTtq1atuuNK7XXr1uXUqVNcvXo1ue3AgQO4uLhQvHjxVH+2iIiIZF7nrp1j/Nbx1PmmDqXGleLttW+z98I+PJKg3V6DNwu2AQMshgUbJk8EP+HskkVERNKMwyPpLVq0oE+fPnzzzTc8/PDDAPz666/07duXUAf3De3fvz+dO3emZs2a1K5dm6+//ppjx47Rt29fwD5V/eTJk0yfPh2ATp06MXz4cLp3786wYcM4f/48r7/+Oj169LjjwnEiIiKS+V1NuMqCfQuYtWsWKw+tTF6Z3cVw4YmLeXlm3UXaRHrgO2seNGtG7f1hWgBORESyJYdD+rhx4+jatSu1a9fGzc0NgKSkJEJDQxk7dqxDfXXo0IELFy7w3nvvERUVxYMPPsjSpUspUaIEAFFRURz7e79TgNy5c7Nq1Sr+7//+j5o1a1KgQAHat2/P+++/7+jXEBERESdLtCby3rr3+GHPD0RejiTBmpB8rmaxmjxTui0dhs+n6Npt4O0NixZB/fqAFoATEZHsy+F90m84ePAge/fuxTRNKlSoQOnSpdO6tnShfdJFREScxzRNNp/YzMw/ZjLjjxlcSbj5CF3R3EXpU6MPnSp14gEKQOPGsGMH+Pra90J/5BEnVi4iInL/0nWf9BtKly6dZYK5iIiIONfec3uZuWsms3bNIvJy5C3nLYaFDhU78G7Iu3D6NDQMgd27oWBBWLkSqlXL8JpFRESc4b5DuoiIiMjdnLpyyr4y+66ZRJyOSG7P7Z6btuXbUjJvSd5d9y4Ww4LVtFI/uD4cOwYNGsDBg1C0KKxeDRUqOPFbiIiIZCyFdBEREUkz0XHRzNs7j+92fcfayLWY2J+qc3VxpUnpJjxT6RlCy4bi5Wbf6Lxa0Wo3F4CzVIB69exBvUQJWLMGSpVy5tcRERHJcPf9THpWpWfSRURE0t4Xv37B2K1jOXL5CEm2pOT2ugF1eabSM/yv4v8o6FXwzh3s2QNPPglRUVCmjD2gBwRkQOUiIiLpL92eSU9KSuKDDz6gR48eBOg/nCIiIgJ8veNr/m/5/yUfF89TnOcfep6OD3YkOF/wvTuIiIBGjeD8eXjwQVi1CooUSceKRUREMi8XRy52dXXl448/xmq1plc9IiIikoVcjL3Im6vfTD62GBbaVWjHW/XeSl1Af/99+6rt589DzZoQHq6ALiIiOZpDIR3gySefJDw8PB1KERERkawkPimeNnPacCnuEkDKBeDuxjTt09krV4Z33oGEv/dH798fChRI56pFREQyN4cXjmvatCmDBg1i9+7d1KhRA29v7xTnQ0ND06w4ERERyZxM06RnWE/WH12Pj4cPw+sP58jlI/YF4Mre4e8CNhssWgQjRsDWrSnPWSywbRt07Jj+xYuIiGRiDi8c5+Jy58F3wzAy/VR4LRwnIiLy3w1ZO4Th64fj6uLKsmeW8WTJJ+98cVISzJ0LI0fa9z4H8PS0b7W2ZIk9oFutsHAh6B/7RUQkG0q3heMAbDbbfRcmIiIiWd/UiKkMXz8cgIktJt45oMfHw7ffwkcfweHD9jYfH3jxRXjlFShcGMLC7M+hh4QooIuIiPAf90mPi4vD09MzrWoRERGRTG714dX0WdwHgMH1BtOjWo9bL7p6Fb7+GkaPhlOn7G0FC9qD+YsvQt68N68NDVU4FxER+QeHQ7rVamXEiBF89dVXnDlzhgMHDlCyZEneeecdgoKC6NmzZ3rUKSIiIk62++xunpr7FEm2JDpV6sTw+sNvngwLg2XLICYGVqyACxfs7f7+8Prr0KsX/GsdGxEREbmVw6u7f/DBB0ybNo1Ro0bh7u6e3F6pUiUmT56cpsWJiIhI5hB1JYrms5oTEx9DvcB6TAmdgmEY9pMLFkCrVvDVVzBrlj2gly4NkyfDoUPw8ssK6CIiIqnkcEifPn06X3/9Nc888wwWiyW5vXLlyuzbty9NixMRERHnu5ZwjZazW3Is+hgPFHiA+R3m4+HqYT958iT065fyhmbNYN8+6NkTPDwyvmAREZEszOGQfvLkSUqXLn1Lu81mIzExMU2KEhERkczBarPS8aeO7IjaQUGvgizttJQCXn/vZT5/vn2v8+PH7cc3doB57jn7iu0iIiLiMIefSa9YsSIbNmygRIkSKdp/+OEHqlWrlmaFiYiIiHOZpskry19h0YFFeFg8CHs6jFL5S8G1a9C/v31xOIAaNeyj5n/9pVXaRURE/iOHQ/rQoUPp3LkzJ0+exGazMW/ePPbv38/06dNZvHhxetQoIiIiTjD217F8se0LAL5r+x21A2pDRAR07Aj794NhwMCB8N578I91akREROT+OTzdvWXLlsyZM4elS5diGAZDhgxh7969LFq0iIYNG6ZHjSIiIpLBFuxbQP8V/QH4uOHHtCvXFj75BGrVsgf0YsVg9Wr48EMFdBERkTR0X/ukN27cmMaNG6d1LSIiIpIJbD25lU4/dcLEpG+NvrxWoiM0bmwP5QBt2sCkSVCggHMLFRERyYbuK6QDbN++nb1792IYBuXLl6dGjRppWZeIiIg4waTfJvHyspeJTYqlaemmfG5thFGlin1btVy5YMwY6N3bPtVdRERE0pzDIf3EiRN07NiRjRs3kjdvXgAuX75MnTp1mD17NgEBAWldo4iIiGSAhfsW0mdRn+TjrtsTcR3T1n5QrZp9D/Ry5ZxUnYiISM7g8DPpPXr0IDExkb1793Lx4kUuXrzI3r17MU2Tnj17pkeNIiIikgG+3vF18nuLDX7d+/f09gEDYPNmBXQREZEM4PBI+oYNG9i0aRNly5ZNbitbtiyff/45devWTdPiREREJGNcTbjKr5EbADBsYHWBkHPesHI+aGFYERGRDONwSA8MDCQxMfGW9qSkJPz9/dOkKBEREclYH6z/gAvWKxS+Ch12w5OREFq/swK6iIhIBnN4uvuoUaP4v//7P7Zv345pmoB9EbmXX36ZTz75JM0LFBERkfS1//x+Rm8eDcCkRTBuhUHofqBpU+cWJiIikgMZ5o2kfRf58uXD+McqrteuXSMpKQlXV/tA/I333t7eXLx4Mf2qTQMxMTH4+voSHR2Nj4+Ps8sRERFxKtM0afJtQ1YeXUOzA7B4UwmM5i2gUSMIDXV2eSIiItmCIzk0VdPdx4wZkxZ1iYiISCazYM88Vh5dg3sSjN1WAGPdeggMdHZZIiIiOVaqQnrXrl3Tuw4RERHJYNcTr/PK3B7gAq//aqH09MUK6CIiIk7m8MJxN5w9e5azZ89is9lStFeuXPk/FyUiIiLpb+T4pznmEkPgZXiryyR45BFnlyQiIpLjORzSd+zYQdeuXZP3Rv8nwzCwWq1pVpyIiIikj4MrZjPqwiJwhc/ytMPr2e7OLklERES4j5DevXt3HnjgAb755hv8/PxSLCgnIiIiWcDhw7wyuysJwdDwqh9tPvze2RWJiIjI3xwO6ZGRkcybN4/SpUunRz0iIiKSnqKjWdQnhCX1EnGzGXz+8nIMi8XZVYmIiMjfHN4nvUGDBvz+++/pUYuIiIikp6Qk4jr+j5cfPA5A/2ovULZ4VefWJCIiIik4PJI+efJkunbtyu7du3nwwQdxc3NLcT5Ue6qKiIhkTq+9xqjrq4jMB/6ehXm72YfOrkhERET+xeGQvmnTJn755ReWLVt2yzktHCciIpJJffUVkdPHMfJF++Ho5uPI7Z7buTWJiIjILRye7t6vXz86d+5MVFQUNpstxUsBXUREJBNaswZeeolXm0CcGzwR/ATtK7Z3dlUiIiJyGw6H9AsXLvDqq6/i5+eXHvWIiIhIWjpwANq1Y1mwlYXlwNXFlc+bfq7dWURERDIph0N627ZtWbt2bXrUIiIiImnp4kVo0YL4K5fp18YTgJdrvUyFQhWcXJiIiIjcicPPpD/wwAMMGjSIX375hUqVKt2ycFy/fv3SrDgRERG5T4mJEBICf/3F6MZeHPS+TpHcRRjy+BBnVyYiIiJ3YZimaTpyQ3Bw8J07MwwOHz78n4tKTzExMfj6+hIdHY2Pj4+zyxEREUkfzz0HX3/NMV8o9xLEusF3bb7jmcrPOLsyERGRHMeRHOrwSHpkZOR9FyYiIiIZwDRh3jwAXmtkD+j1kvzpVKmTkwsTERGRe3H4mXQRERHJ5CIi4Px5VpeEHyuCxQZfPDhQi8WJiIhkAQ6PpPfo0eOu56dMmXLfxYiIiEgamDKFBAt0a+8OJNA0b00qd9CaMSIiIlmBwyH90qVLKY4TExPZvXs3ly9f5oknnkizwkREROQ+xMXBrFn0bQ4nPRMAWByznbD9YYSWDXVycSIiInIvDof0+fPn39Jms9l44YUXKFmyZJoUJSIiIvdp4UIuxF1iVuWbTRbDQviRcIV0ERGRLCBNnkl3cXHh1Vdf5bPPPkuL7kREROR+TZ3Ke49D/N//DG8xLFhNKyFBIU4tS0RERFLH4ZH0Ozl06BBJSUlp1Z2IiIg46vhxDmxfwYQX7IfDQoZxOe4yIUEhGkUXERHJIhwO6f37909xbJomUVFRLFmyhK5du6ZZYSIiIuKg6dN540lIskCzMs0Y8vgQZ1ckIiIiDnI4pEdERKQ4dnFxoVChQowePfqeK7+LiIhIOjFN1i2ZwILGYMGFjxt+7OyKRERE5D44HNLXrl2bHnWIiIjIf2Bbv47XKp0CoHeV7lQoVMHJFYmIiMj9SJOF40RERMS5Zs19hx3FII/NjWENRzi7HBEREblPqR5Jr1+/PoZh3PUawzBYs2bNfy5KREREUi/24lne8twIwKAHelLYu7CTKxIREZH7leqQXrVq1Tuei4mJYfbs2cTHx6dFTSIiIuKAz77ty3Efk8Brrrzyv9HOLkdERET+g1SH9NvtgZ6UlMT48eP54IMP8Pf3Z/jw4WlanIiIiNzdmatnGHlxIbjCiLztyOXu5eySRERE5D+4733SZ86cyZAhQ4iNjeXdd9+lT58+uLqm2bbrIiIikgpDF7zMVVcbNU9Bx6GfOLscERER+Y8cTtXLly/nzTffJDIykgEDBtC/f3+8vb3TozYRERG5iz/P/smkg3PBgE9jauNSzN/ZJYmIiMh/lOqQvnXrVt544w22bNlC3759Wb16NQULFkzP2kREROQuXl85AJth0mYv1Gs/wNnliIiISBowTNM0U3Ohi4sLuXLl4rnnniMoKOiO1/Xr1y+taksXMTEx+Pr6Eh0djY+Pj7PLERERuS+rDq2i0XeNcLXCnln5KLPnNLi7O7ssERERuQ1HcmiqR9IDAwMxDIP58+ff8RrDMDJ9SBcREcnqrDYrr618DYAXt0GZll0V0EVERLKJVIf0I0eOpGMZIiIiklrTdk5j19ld5I2FIeuAUT2cXZKIiIikERdnFyAiIiKpdzXhKm+vfRuAd9ZD/go1oFIlJ1clIiIiaUUhXUREJAv5eOPHnL56mpJX3XlxK9BDo+giIiLZiUK6iIhIFnEy5iQfb/oYgI+WJuDh6gEdOzq5KhEREUlLCukiIiJZxNtr3yY2KZa6iUV5ag/Qpg3ky+fsskRERCQNKaSLiIhkATtP7+Tbnd8CMPqnKxgA3bs7tSYRERFJe/cV0g8dOsTbb79Nx44dOXv2LADLly/nzz//TNPiREREBEzT5LWVr2Fi8nSeOtTadxUCAqBBA2eXJiIiImnM4ZC+bt06KlWqxK+//sq8efO4evUqAH/88QdDhw5N8wJFRERyuiV/LeHnyJ/xsHgwMtxib+zaFSwW5xYmIiIiac7hkP7mm2/y/vvvs2rVKtzd3ZPb69evz+bNm9O0OBERkZwu0ZpI38V9AWhWvD5Bi3+xn+jWzXlFiYiISLpxOKTv2rWLNm3a3NJeqFAhLly4kCZFiYiIiF2b79tw8spJAOYfXU7YAyaEhECpUs4tTERERNKFwyE9b968REVF3dIeERGBv79/mhQlIiIi8Pmvn7Pk4JLkY4sNwoPQgnEiIiLZmMMhvVOnTrzxxhucPn0awzCw2Wxs3LiRAQMG0KVLl/SoUUREJMdZ+tdSXlnxSvKxBResLhByJhc89ZTzChMREZF05XBI/+CDDwgMDMTf35+rV69SoUIFHnvsMerUqcPbb7+dHjWKiIjkKH+c+YMOP3bAZtroWa0nCzosoF90ORbOhtCaz4C3t7NLFBERkXRimKZp3s+Nhw4dIiIiApvNRrVq1ShTpkxa15YuYmJi8PX1JTo6Gh8fH2eXIyIiksLpq6d5eNLDHI85Tv2g+ix/djnu1+KgaFG4fh02bYLatZ1dpoiIiDjAkRx6X1uwAZQqVYp27drRvn37/xTQJ0yYQHBwMJ6entSoUYMNGzak6r6NGzfi6upK1apV7/uzRUREMpPridcJnR3K8ZjjlC1Qlp/a/4S7xR3mzrUH9LJl4ZFHnF2miIiIpCOHQ3rDhg0JDAzkzTffZPfu3f/pw+fMmcMrr7zC4MGDiYiIoF69ejRt2pRjx47d9b7o6Gi6dOlCgwYN/tPni4iIZBY200aX+V3YdmobBXIVYHGnxeTLlc9+cvRo+89HHgHDcF6RIiIiku4cDumnTp1i4MCBbNiwgcqVK1O5cmVGjRrFiRMnHP7wTz/9lJ49e9KrVy/Kly/PmDFjCAgI4Msvv7zrfc899xydOnWitqb7iYhINjF4zWB+2vsTbi5uzO8wn9L5S9tPDBwI+/bZ33/7LYSFOa9IERERSXcOh/SCBQvy0ksvsXHjRg4dOkSHDh2YPn06QUFBPPHEE6nuJyEhgR07dtCoUaMU7Y0aNWLTpk13vG/q1KkcOnSIoUOHpupz4uPjiYmJSfESERHJTKZGTOXDjR8C8E3oN9QrUQ+uXoVeveDjj29eaLFAeLhzihQREZEM4XBI/6fg4GDefPNNPvzwQypVqpT8vHpqnD9/HqvVip+fX4p2Pz8/Tp8+fdt7/vrrL958801mzpyJq6trqj5n5MiR+Pr6Jr8CAgJSXaOIiEh6Wxu5lj6L+wDwzmPv0LlKZ9i+HapXh2++uXmhxQJWK4SEOKdQERERyRD3HdI3btzICy+8QNGiRenUqRMVK1Zk8eLFDvdj/OvZOtM0b2kDsFqtdOrUiWHDhvHAAw+kuv9BgwYRHR2d/Dp+/LjDNYqIiKSHAxcO8NTcp0iyJdGhYgeGPTYURo2yr97+119QvLh95HzhQujXz/4zNNTZZYuIiEg6St1w9D+89dZbzJ49m1OnTvHkk08yZswYWrdujZeXl0P9FCxYEIvFcsuo+dmzZ28ZXQe4cuUK27dvJyIigpdeegkAm82GaZq4urqycuXK20639/DwwMPDw6HaRERE0tuF6xdoPqs5l+Iu8UjxR5j60PsYjRrBzz/bL2jXDiZOhPz57ccK5yIiIjmCwyE9PDycAQMG0KFDBwoWLHjfH+zu7k6NGjVYtWoVbdq0SW5ftWoVrVq1uuV6Hx8fdu3alaJtwoQJ/Pzzz/z4448EBwffdy0iIiIZKT4pnjZz2nDw4kFK+JZgQZ4+5KpeCy5eBC8v+Pxz6N5dK7mLiIjkQA6H9Lst6uao/v3707lzZ2rWrEnt2rX5+uuvOXbsGH379gXsU9VPnjzJ9OnTcXFx4cEHH0xxf+HChfH09LylXUREJLMyTZM+i/uw4dgGfNx9WPLXQ/i92sN+skYNmDULHHisS0RERLKXVIX0sLAwmjZtipubG2H32Pol1IHpeB06dODChQu89957REVF8eCDD7J06VJKlCgBQFRU1D33TBcREclKRv4ykum/T8diWJi7woeK6360j5gPHAjvvQfu7s4uUURERJzIME3TvNdFLi4unD59msKFC+Picue15gzDwGq1pmmBaS0mJgZfX1+io6Px8fFxdjkiIpKDDFw1kI832bdUm7DMhed/tYG/P8yYAfXrO7k6ERERSS+O5NBUjaTbbLbbvhcREZG7S7QmsvjAYoavH07E6Yjkdv/LNmjTBiZNggIFnFihiIiIZCYOb8E2ffp04uPjb2lPSEhg+vTpaVKUiIhIVnf40mHeWvMWgWMCaTu3bYqAbrFBeKsq8NNPCugiIiKSgsMhvXv37kRHR9/SfuXKFbp3754mRYmIiGRFidZEftrzE41mNKLUuFKM/GUkp6+exu8qPPWn/RqLDawuEGKW0OrtIiIicguHV3c3TRPjNn+pOHHiBL6+vmlSlIiISFZy8OJBJv82mWk7p3Hm2hkADBMaHTbos92k5X5wCy5F2B+HCA82CIk0CR3V08lVi4iISGaU6pBerVo1DMPAMAwaNGiAq+vNW61WK5GRkTRp0iRdihQREclMwvaHsfrwanK55mJH1A7WRK5JPlckyZOem+PouQOCL5vw2GMwZjA0bEjookWEhofD8yHgwG4oIiIiknOkOqS3bt0agJ07d9K4cWNy586dfM7d3Z2goCCeeuqpNC9QRETEma4lXON4zHGORx/nWPQxVh9ezfd/fp/iGgODJtGF6LPsLM0PxOFmA5o1g0GD4NFHb14YGqpwLiIiIneV6pA+dOhQAIKCgujQoQOenp7pVpSIiEhGmLd3HksOLKFMgTIE+gYmB/HjMTd/Xoy9eNc+asX4MOebGEpEn7U/Y/6//9nDedWqGfMlREREJFtx+Jn0rl27pkcdIiIi92/cOPtK6aVKwYMPAvY1VM6aVzluXua47TLHzEsct13muHmZY7bLHLCd4yLXU9W9D54EuuQlwMiLDZMV1v242MDmAm8tiaHENVfo0QUGDoSyZdPzm4qIiEg253BIt1qtfPbZZ8ydO5djx46RkJCQ4vzFi3cfcRAREUkL0XHRHN+wmGNjhrGYv9jxAHgnrMf8A477wgkfiE/tf+VM8I+BJ45AQDQERkNAzN8/o8E3Pg44/fcLwspCeBCEHIHQ0s1h5QQIDEyX7ykiIiI5i8MhfdiwYUyePJn+/fvzzjvvMHjwYI4cOcKCBQsYMmRIetQoIiI5RNj+MNZGrqVOQB2qFKnC8ejjN6ee//3+eMxxjl88whXr36PgD9+5P8OEItZcBCZ5E5DkRUCSF4GJ3gQkeXPk9F4GVDuXvCXahN/9CS1WH7yBYncpcvt2QvfvJ3S/CRYLNHtAAV1ERETSjGGapunIDaVKlWLcuHE0b96cPHnysHPnzuS2LVu2MGvWrPSqNU3ExMTg6+tLdHQ0Pj4+zi5HRCTHuZ54nZMxJzkec5wTMSeSX9tObWP7qe0O9ZX/Ori4unHBLRHTAMMGjX2rM+ipzwj0DaRYnmK4W9xvf3NYGGEDW/1jS7SFqVvULSwMWrWyB3SrFRam8j4RERHJsRzJoQ6HdG9vb/bu3UtgYCBFixZlyZIlVK9encOHD1OtWjWio6P/U/HpTSFdRCT9XEu4lhy6F+1fxNZTW/Hx8MHVxZUTMSdStRDbDa4urpTJX4YAn+IEnE8gYPMeAg6eIyAaAuI9CHiqB96vvUlY7E5afd8Ki2lgNUwWPr2Q0LKpDM1hYRAeDiEhjgXt+71PREREciRHcqjD092LFy9OVFQUgYGBlC5dmpUrV1K9enW2bduGh4fHfRctIiKZ040p6LWK16KyX+UUo983XjdGxS/HXU5Vn95u3gT4BlDcpzjFfYoT4BPAhesXmLB9AhbDgtW08mOrWbTafBGGfgSRkfYbfXzgxRfhlVegcGEAQglk4dMLCT8STkhQSOoDOtz/lmjaSk1ERETSicMhvU2bNqxZs4ZatWrx8ssv07FjR7755huOHTvGq6++mh41iohkT2FhsHYt1K/v+ChuGt5nmiaX4i6lCN0nY05yIuYEv53+jZ2nd4IJGKn7mDzueXCzuCWPmBsYNAhuwGt1XksO5b4evhjGrR02vpiX8N1LCLlakNAmL0NUlP1EwYLw6qvwwguQN+8t94WWDXUsnIuIiIhkUg5Pd/+3LVu2sGnTJkqXLk1oFhhV0HR3EckUFiyANm1uHgcEQJ48977vyhU4fjzV99kwOe9h5YQZzU/5z7C5OBSIhVzuXpzwgRNeSZzIlUisa+r+U+CRBGWuelA81pWA624Uj3WjeKwrxW+8v+6KT5KFsHxnadXwfPKibAtXFST0UmHHvhuAvz+8/jr06gXe3qmqUURERCSzSddn0rM6hXQRcbrERKhUCfbvv6/bw8rC2iB4/Ag8dMq+1didXid9INGSun4LXoPiMSlf57xgbG1uhu3ZEJrKslNsU3Y/X7VhQ1i8GNzvsPCbiIiISBaR5s+kh4WFpfrDs8JouoiI08THQ4cONwO6iwvYbDB8ONStm3yZaZpcSIrhePxZjsWd5Vj8GY7Fn2XziS1s5BiYMKZ26j7SwMALV66ZiWDYtyWra5Tg+XLPUty9IP4eBfH3KISny23C8MaNPPHdO4QHQ0gkhD6bss472riR0HfeIfSv23+/O93DO+/c/DV56SUFdBEREclxUjWS7uLikrrODAOr1fqfi0pPGkkXEae5ft0+xX3lSvDwYNbApvxk3U3xYuUo8OBDHIs+luIVmxR7zy4NjBQLsPnn8U9+f+NVNHdRlh1cljVWQNeq6SIiIpINabr7XSiki4hTxMRAixaYGzaw4QEPhvQtx7qY3+95m5+3H4G+gcmvmPgYvon4BhfDBZtpY377+bQu3zpVJYTtD7u/FdBFRERE5D9RSL8LhXQRyXAXL3Ku1ZN8a4tgck0X9ue3pThtYFCxUEXaV2yfHMZvjI57unre0p3CtoiIiEjWkq4h/b333rvr+SFDhjjSXYZTSBeRjGIzbfwc8ROTvuzN/KLRyQu4ebt5UyegDqsOr0reE9yh6eciIiIikqWka0ivVq1aiuPExEQiIyNxdXWlVKlS/Pbbb45XnIEU0kUkvUVdiWLqzql8s20ih68cS26vma8ifeq+zNMPPk0ejzwaERcRERHJIdJ8dfd/ioiIuO0HduvWjTb/3PNXRCSHCNsfxprINfh6+LLr7C4W7V+E1bQvoukTB88ezk3v176j6sOtUtwXWjZU4VxEREREUkizZ9J3795NixYtOHLkSFp0l240ki4iaWnazml0X9j9lvY6p93pszmB/8WVxGvFz1CihBOqExEREZHMIF1H0u/k8uXLREdHp1V3IiKZlmmarD+6ngnbJ/DDnz+kOFfNtxwzvjpDxf2XoEIFWLsaihZ1UqUiIiIiktU4HNLHjRuX4tg0TaKiopgxYwZNmjRJs8JERDKbmPgYZvw+gwnbJ7Dn3J4U525sifbujONU3H8NqlWz74desKCTqhURERGRrMjhkP7ZZ5+lOHZxcaFQoUJ07dqVQYMGpVlhIiKZxa4zu/hy+5fM+GMGVxOuAvYV2p+t/CzP13yeo6vmEr71B0LCjxC6+xrUqQNLlkDevM4tXERERESyHIdDemRkZHrUISKSqSRYE5i3dx4Ttk1gw7ENye3lCpbjhZov0KVKF3w9fWHBAqp0GUHy8m+VKsGKFZA7t1PqFhEREZGsLc2eSRcRyerC9ocRtj+MmPgY1h1dx9lrZwGwGBbalG/DCzVfICQoBMMwICEBvvkGBgy42YFhQEiIArqIiIiI3DeHQ3pcXByff/45a9eu5ezZs9hsthTnM/s+6SIi/3Y14Srv/PwOY34dk6K9aO6iPFfjOXpV74W/j7+98fp1mDwZPv4YTpy4ebFhgGnCk09mXOEiIiIiku04HNJ79OjBqlWraNeuHQ8//LB9RElEJItJtCay8tBKZu6ayYJ9C4hNik1xvnmZ5szvMB83i5u94fJlmDABPvsMzp+3txUtah9J9/eHX3+1j6KHat9zEREREbl/Dof0JUuWsHTpUurWrZse9YiIpBvTNNlyYgszd81kzp9zOH/9fPK5ormLEnU1CothwWpa6VOjjz2gnz0LY8fCF19ATIz94pIl4Y03oEsX8PS0t3Xo4IRvJCIiIiLZjcMh3d/fnzx58qRHLSIi6WLf+X3M/GMms3bP4vClw8nthb0L83TFp3mm8jM8VOwhFh1YRPiRcEKCQgj1qgYvvwyTJkHs36PsFSvCoEH2QO6qJT1EREREJO05/LfM0aNH88Ybb/DVV19RokSJ9KhJROQ/mzb1ZaYe/omTXlYOJZxObvd286ZN+TY8U+kZniz5JK4uN/8YDN0PofMuwsnPYd06SEy0n3joIRg8GFq2BBeXjP4qIiIiIpKDOBzSa9asSVxcHCVLlsTLyws3N7cU5y9evJhmxYmIOOrC1XN0Hl6NZV4n7X/CJYCLDZocgmf/MAg9cB3vxFnArJQ3mqb99U/169vD+RNP2BeGExERERFJZw6H9I4dO3Ly5ElGjBiBn5+fFo4TkUwhyZbEV6s/ZMj6YVzySkpud7FB7x3w1RKAGyHcvF0XNxkGtG8P33+fXuWKiIiIiNyWwyF906ZNbN68mSpVqqRHPSIiDlt9eDWvzO3Bn/HHwQ1KXIajecFiA6sLNGv+Mkx+8+6drFgB3bqBxQJWK3TqlAGVi4iIiIik5HBIL1euHLGxsfe+UEQknR26eIjXlr3CwoOLAShwHd4/GEivD5axdNtMwvcuJ6R8E0I7f3Dvzrp2hXz5IDxcW6mJiIiIiNMYpvnvhzDvbuXKlQwbNowPPviASpUq3fJMuo+PT5oWmNZiYmLw9fUlOjo609cqIrd3Jf4KI38ZyehNn5BgS8Rigxe3wbuV+pFv+Mfg7u7sEkVEREREkjmSQx0O6S5/r2z872fRTdPEMAysVquD5WYshXSRrMtm2vjuj+94c/WbRF2NAqDhIRizoxAVPv/evsCbiIiIiEgm40gOdXi6+9q1a++7MBGR+/XriV95efnL/HryVwBKXYRPV0DLCq0x1k+GAgWcXKGIiIiIyH/ncEh//PHH06MOEZHbmrZzGqM2jmLv+b0A5E6At9fBK7974jF6LPTure3RRERERCTbcDikr1+//q7nH3vssfsuRkTkBqvNSt/FfZkcMTm5rf5hmDkPipapBttmQblyTqxQRERERCTtORzSQ0JCbmn75/Ppmf2ZdBHJ/CKiIuizuA/bT21PbnOxQdUzULTvAHj/ffDwcGKFIiIiIiLpw8XRGy5dupTidfbsWZYvX85DDz3EypUr06NGEckhriVcY8DKATw06SG2n9qOl9X+R5TFBjYXCKndET7+WAFdRERERLIth0fSfX19b2lr2LAhHh4evPrqq+zYsSNNChORnGXpX0t5YckLHI0+CkD7P2HMMhvb/CE8CEKOuRDapIhzixQRERERSWcOh/Q7KVSoEPv370+r7kQkhzh99TQvL3+ZuX/OBaDEZZiwBJr9BdSoQeiOHYQetIDVCh+GOLNUEREREZF053BI/+OPP1Icm6ZJVFQUH374IVWqVEmzwkQke7OZNibt+Jo3lg8g2noNFxu8ugWGhYN36/YwdxBUrQphYRAeDiEhEBrq3KJFRERERNKZwyG9atWqGIaBaZop2h955BGmTJmSZoWJSPb15+ldPPddBzZes2+rVuMUfL3UQvVGXWHXG/DAAzcvDg1VOBcRERGRHMPhkB4ZGZni2MXFhUKFCuHp6ZlmRYlI9hQXd5X3v36WURfCSHQx8U6A9ze48VK153Dd+DoEBjq7RBERERERp3I4pJcoUSI96hCR7Mxm4/036zDa8iuXPQEXaHnIlS+K9yHwx6FQuLCzKxQRERERyRRSvQXbzz//TIUKFYiJibnlXHR0NBUrVmTDhg1pWpyIZANRUfTuWYh3vP8O6MAbFyuycOxZAkeMV0AXEREREfmHVIf0MWPG0Lt3b3x8fG455+vry3PPPcenn36apsWJSNZmWxTG631LMTnoYnKbxQYJ3h4Y+fI5sTIRERERkcwp1SH9999/p0mTJnc836hRI+2RLiJ2sbEkvPQ8Xaa14pPqscnNFhtYXSCk/J3/LBERERERyclS/Uz6mTNncHNzu3NHrq6cO3cuTYoSkSzsjz+I6dKBpyrvY3VlcDVd+KbF1+Tde5jwvcsJKd+E0M4fOLtKEREREZFMKdUh3d/fn127dlG6dOnbnv/jjz8oWrRomhUmIlmMacK4cUS9N5Bm/0tgZ1HwdvHkx47zaVK6CdSEUBTORURERETuJtXT3Zs1a8aQIUOIi4u75VxsbCxDhw6lRYsWaVqciGQRZ85As2bsH/4KdTrbA3rhXAUJ77nBHtBFRERERCRVDNM0zdRceObMGapXr47FYuGll16ibNmyGIbB3r17GT9+PFarld9++w0/P7/0rvk/iYmJwdfXl+jo6NsugiciDlq6FLp3Z4v7WVp0ggteUDp/aZY/s5xS+Us5uzoREREREadzJIemerq7n58fmzZt4vnnn2fQoEHcyPaGYdC4cWMmTJiQ6QO6iKShuDgYOBA+/5xFD0CH9gaxriYPFXuIxZ0WU9hbW6uJiIiIiDgq1SEdoESJEixdupRLly5x8OBBTNOkTJky5NNWSiI5y+7d0KkT7NrFpOrQt6WBzTBpWropc/83l9zuuZ1doYiIiIhIluRQSL8hX758PPTQQ2ldi4hkBS+9BF99hWm1Mqy5F8Meug6Y9Kjag69afIWb5c67QIiIiIiIyN2leuE4ERHGj4fx40kyrfRpyd8BHd6u9zaTQycroIuIiIiI/Ef3NZIuIjnUhAn8UAHeeBIi84OLaTC+xQT61uzr7MpERERERLIFhXQRSZ0TJ5jHXtq3v9n0ht9TCugiIiIiImlI091FJHU++4xPH7m5Y6MLBnElA5xYkIiIiIhI9qOQLiL3dukSh+Z8xdZi9kMXwwUbJiFBIU4tS0REREQku9F0dxG5J3P8ePo+cZ1EV6hcuDJPBD9B/eD6hJYNdXZpIiIiIiLZikK6iNxdbCwzl49idUPwNNz4qcNPlM5f2tlViYiIiIhkS5ruLiJ3df6bz3m1zhUA3nl8iAK6iIiIiEg6UkgXkTtLSuL1X4dz3hsqWooy4NGBzq5IRERERCRbU0gXkTv6+dt3mVb6KoYJkzrMxN3i7uySRERERESyNYV0EbmtuMRY+u77BIC+xkPULlPfyRWJiIiIiGR/CukiclsffNuTv3LHU/Sqwcjec5xdjoiIiIhIjuD0kD5hwgSCg4Px9PSkRo0abNiw4Y7Xzps3j4YNG1KoUCF8fHyoXbs2K1asyMBqRXKGP8/+yYfHvwfgc5riWyzYyRWJiIiIiOQMTg3pc+bM4ZVXXmHw4MFERERQr149mjZtyrFjx257/fr162nYsCFLly5lx44d1K9fn5YtWxIREZHBlYtkXzbTRp/vO5HkYtLygEHblyY4uyQRERERkRzDME3TdNaH16pVi+rVq/Pll18mt5UvX57WrVszcuTIVPVRsWJFOnTowJAhQ1J1fUxMDL6+vkRHR+Pj43NfdYtkZxO3T6Tvkr7kjoc9p58iYMqPzi5JRERERCRLcySHOm0kPSEhgR07dtCoUaMU7Y0aNWLTpk2p6sNms3HlyhXy589/x2vi4+OJiYlJ8RKR24u6EsUbK18H4P2fIeC1YU6uSEREREQkZ3FaSD9//jxWqxU/P78U7X5+fpw+fTpVfYwePZpr167Rvn37O14zcuRIfH19k18BAQH/qW6R7Ozl5S8TnXiFmifhpULNoWJFZ5ckIiIiIpKjOH3hOMMwUhybpnlL2+3Mnj2bd999lzlz5lC4cOE7Xjdo0CCio6OTX8ePH//PNYtkR4sPLOaHPT9gscGkRWB5c5CzSxIRERERyXFcnfXBBQsWxGKx3DJqfvbs2VtG1/9tzpw59OzZkx9++IEnn3zyrtd6eHjg4eHxn+sVyc6uJlzlxaUvAvDqZqhaqi7UrevkqkREREREch6njaS7u7tTo0YNVq1alaJ91apV1KlT5473zZ49m27dujFr1iyaN2+e3mWK5AhD1g7hWPQxgqIN3g0H3njD2SWJiIiIiORIThtJB+jfvz+dO3emZs2a1K5dm6+//ppjx47Rt29fwD5V/eTJk0yfPh2wB/QuXbowduxYHnnkkeRR+Fy5cuHr6+u07yGSle04tYOxv44F4MtFJt5lKoD+AUxERERExCmcGtI7dOjAhQsXeO+994iKiuLBBx9k6dKllChRAoCoqKgUe6ZPnDiRpKQkXnzxRV588cXk9q5duzJt2rSMLl8ky0uyJdF7UW9spo2Of3nS5GAcTBsILk5frkJEREREJEdy6j7pzqB90kVuGr1pNANWDSCv4cW+Udfxyx8ABw+Cu7uzSxMRERERyTayxD7pIuJck36bxJtr3gTg483e+F0D+vdXQBcRERERcSKnTncXEeeY+cdM+izqk3xcMPIc5MsHvXo5sSoREREREdFIukgOs/3Udl5Y+kLyscUG64OAl16C3LmdVpeIiIiIiCiki+QYpmny1favqDulLjHxMQBYcMHqAiGn3OH//s/JFYqIiIiIiEK6SA5wLeEaned35vklz5NgTaBV2VbMbDuTfieLs3A2hD7eBwoVcnaZIiIiIiI5np5JF8nm9p3fR7u57fjz3J9YDAsjG4xkQJ0BGGPG0GnSMft2a8tfc3aZIiIiIiLy/+3de1xUZeLH8e/McFG8oOIVRfCSeKlUdDU1Ei+puTq1ZmJe62eZ2a6pmWvZeiuz3crMUmvL1MwLq2WS61bkipc0LaQyNbzfVtHQAgQVmDm/PybHULxAwBmYz/v1mpfy8Bzme177RPvtOeeMKOlAqRbzQ4we+eQRncs6p5rlayqmb4zuCr1LWrnS9SR3SXI6pe+/l8LCTM0KAAAAgMvdgVIpy5GlUf8Zpf4f9te5rHOKCotS4mOJroK+e7f0+OOXJ9tsUny8aVkBAAAAXEZJB0qZo6lHddeCu/TG9jckSRM6TFDc4DjVLFdDmjdPatVKSklxTbZaJYdDiooyLzAAAAAANy53B0qRz/Z/poEfDdSZ82dUqUwlvX/f++od3ttVyocNk2JjXRO7dZMGDJC++85V0O12U3MDAAAAcKGkA6WAw+nQtA3T9PzG52XIUEStCK14YIXqV64vxcVJQ4ZIycmSn5/00kvSk0+6dtEBAAAAeBRKOlDCvf/d+/rb+r/paOpRSdLwiOF6/Z7XVcZhkcaNk1591TWxSRNp6VKpRQvzwgIAAAC4Lko6UEIln0vW42se18dJH7vHnmz7pGb1mCX9+KP04IPSt9+6vvH449Irr0gBAWZEBQAAAHCTKOlACXMs9Zhe3vKy3tnxji7kXHCP2yw2WWWR3n5bGjNGOn9eCgqS5s+X7r3XxMQAAAAAbhYlHSghDv58UC9tfkkLv12obGe2JKlRUCPtPbNXNotNDsOhqGVbpfdnuQ7o2lVatEgKDjYvNAAAAIB8oaQDniI2Vlq/XurUKdfT1vf8tEczNs/Q0p1L5TAckqSosCj97a6/qVNYJ33ywXOK3/YvRX2VLHvCNsnXV3rxRWnsWB4OBwAAAJQwFsMwDLNDFKe0tDQFBgYqNTVVFStWNDsOvJ3DIe3cKc2dK73zzuXx4GB9X8dXLzQ7o5V1z8mwuIZ7nAjQxF1BuvOnsq6Bc+ekEycuH1e7tqvsR0QU3zkAAAAAuK789FB20oHidPGi9M030qZN0saN0pYtUmqqJCk2XFofJtVOkzaFnlBs48uH3bdHmrhJan0iU1Jm3j/bYpH+9CcKOgAAAFCCUdKBopSe7irimza5Xtu2uYr6b5Uvr9V3VtV9dxyWxZB719wii6Krd9azYYN0W+cG0hN5/PzNm6Vnn5VsNteu/N13F/kpAQAAACg6lHSgsP3yi/R//yd9+aX000/SlXeUVKsmRUZKkZFKvaOlFhk7NGXj89KFywW9adWm+ij6I4VXDb/+e0VGSs2aSfHxUlRUrnvZAQAAAJQ83JMOFCbDkFq3lnbsuDxWvbrUvbu7mCs8XD/8tEtzts/R4u8XKyM7wz3VIosMGVrdf7Xs4RRuAAAAoDTgnnTALHPn5i7oNps0cKA0c6ayHdlanbRaby4aoQ1HNrinNK3WVE/84QkFlQ3Stv9tU1RYFAUdAAAA8FLspAOF5ccfpZYtpQsXXF//ep948sqFeqfqUb2V8JZOpLuexG6z2HRf4/v05zZ/VsfQjrJYLCYGBwAAAFCU2EkHilt2tjRokKug3323Vg9opaX7PtTJyn76avejynZmS5Kql6uu4RHD9Vjrx1SnYh2TQwMAAADwNJR0oDBMmyYlJEiVK+uNpyI16qtJkp+kX283bx/SXk/84Qnd3+R++fv4mxoVAAAAgOeipAO/15Yt0osvSpKS33xJz+54yv0tiywacNsAfdDnA7PSAQAAAChBrGYHAEq09HRp8GDJ6dT5wf11b+Z8ncs6J8l137khQ/2a9TM5JAAAAICSgp104PcYM0Y6eFBG3RD93z3Z2r53u6qUraIXO7+opDNJPKkdAAAAQL5Q0oGCWr1amj9fslg0bVoXLd+7UD5WH33Y70NFhUWZnQ4AAABACcTl7kBBnDolPfqoJCnmr7005fBCSdJbf3yLgg4AAACgwCjpQH4ZhjRsmPTTT9resaEeKh8nSXqq3VMaFjHM5HAAAAAASjJKOpBf//yn9O9/61iQr+w9ftaFnAvq1aiX/t7172YnAwAAAFDCUdKB/Ni7Vxo7Vuf8pN5PVtOpi2d0W/XbtLTPUtmsNrPTAQAAACjhKOnAzcrJkQYPlvN8pgYNr6rvnCdUvVx1ffLgJ6rgX8HsdAAAAABKAUo6cLOmT5e2b9czf/TX6qop8rf56+PojxVaKdTsZAAAAABKCUo6cDO2bZOef14LW0j/aH1RkjTfPl/tQtqZmwsAAABAqUJJB24kI0MaPFgb6zg0/F7XPzLPRT6ngbcPNDkYAAAAgNKGkg7cyFNP6UDKPvV50Kpsi1N9m/bV1E5TzU4FAAAAoBSipAPXM2mSUhe+rd4DpDNlnGod3FqL7lskq4V/dAAAAAAUPpoGcC3vv6+c6c8r+gFpTzUp2LeKVvdfrQDfALOTAQAAACilfMwOAHikCxekCRN0X7T0WUPJL0eKTe+h4ArBZicDAAAAUIqxkw5cyTBkPPqIHmx/Uv8Odw1l+Uj/ax5maiwAAAAApR8lHbiCMWOGxp1eouW3XR6zyar46ufNCwUAAADAK3C5O/Abzg9XauTWiXq7/eUxm8Umh+FQVFiUabkAAAAAeAdKOvCrnG+26+GY/vqgtWQxpHfs76pauWqKPxyvqLAo2cPtZkcEAAAAUMpR0gFJF48d0oNvdtSqZg75OC1a3Gex+jcfKEmUcwAAAADFhpIOr5eZmqI/vdRCn9e7ID+HRSvuWyJ78wfNjgUAAADAC1HS4dXSzv+iXtObaFP1NAVkS6t7LFTXCAo6AAAAAHNQ0uG1zp4/qx5/v01fl0tRxQvS2nZvqsOdQ8yOBQAAAMCLUdLhlZLPJevuN9roB8sJBWVKn9ebpAj7E2bHAgAAAODl+Jx0eJ2jqUd117w2+iHrmGqlSxv0kCJGTDU7FgAAAACwkw7vsv/sfnV5L0pHM/+n0F+kdce7qMHyd82OBQAAAACSKOnwIrtO71LX97soOeOUGqVIX3zdWCFxqySbzexoAAAAACCJy93hJWZunanW/2yt5IxTuj1Z2vhJVYXEfCpVqGB2NAAAAABwYycdpVpGVoaGrxmupTuXusfGb/NRjWWxUmioickAAAAA4GqUdJQsMTHS2rVSmzZS587XnJbtzNa7Bz/U1N1zdOrCGfe4zSklDOmqge3aFUdaAAAAAMgXSjpKjqeekmbOdP39/ffznOK0SCuaSs91lvYHucZqpEunKrgKusMqRQVFFFNgAAAAAMgfSjo8X1qa9MQT0gcf5B4vU0YqV8795RchWZrQLlMJNRySpOqZFk3aHqBHt+fo05CLig+Too5aZe9xXhpcjPkBAAAA4CZR0uHZtm6VBg6UDh2SLBbJMFxPY3c4XJe+2+3acXKHJnwxQXEH4yRJ5f3K6+n2T2tsu7Eq71deio2V/d57Zd//63EvRZl7TgAAAABwDZR0eCaHQ3rxRWnqVNffQ0OlJUukM2ek+HgpKkr772yq51b2V8yuGEmSr9VXj7d+XBPvmqjq5apf/ll2u7R6tfs42e1mnBEAAAAA3JDFMAzD7BDFKS0tTYGBgUpNTVXFihXNjoO8HDkiDRokbd7s+vrBB6V586TAQMUmxWrN3jU6nnZccQfjlOPMkUUWDbhtgKZ1mqb6leubmx0AAAAArpCfHspOOjxLTIz02GNSaqrrM8znzJEGDZIh6Y1ts/Xkp0/mmt6jYQ/N6DJDLWq2MCUuAAAAABQmSjo8Q3q69Je/SIsWub5u21ZaulS7K1xQTPwU/Wv3v/Rjyo+5DunbpK9W9FthQlgAAAAAKBqUdJhv+3ZpwADpwAHJalXSs4/pX52rK+ZTu3b9tMs9zcfqoxxnjqwWq5yGU4Ob84h2AAAAAKULJR3mcTikv/9dmjxZByrkKKZXoP4VVU3fnZsnbXRN8bX6qnvD7urXtJ/s4XZtOLJB8YfjFRUWJXs4D4ADAAAAULrw4DgUP8NQ7OsjtXrb+3JcyNQP1aWE4Mvf9rH6qGv9ropuFq17w+9V5bKVzcsKAAAAAL8TD46Dx8hyZOnw2YPa/+1/tf/7eO079q22ZB/UjuoOqfHleTaLTZ3rdVa/Zv30p8Z/UlBAkHmhAQAAAMAklPRSKDYpVusPrVenep1u+pLwghxz6bi4g3FqHNRYdQPrav/Z/dqfslf7jyRq39l9OuI8K6flNwdcuSluSF0cIVo2IUHVylW76fcFAAAAgNKIku6hfjj9g17Z8or2n92v+pXqK7xquAwZchrOXC/DyD2278w+rd67WhZZNGvbLN3T8B6FBobmPk6X/+5wOnQs9Zg2H9ssiyHN2jZLLWu2VOWylZXlyFK2I9v1p9P152/HMrMzlZGdIRmSLNc4EYsUkCU1/MWihtaqali1kTKUrTnaLptTclilUY0GU9ABAAAAQNyTbnacaxr9n9F6ffvrZse4eYZULUOKOiI1PCs1vFhODeu20C0tuqjmXT1liYiQfH3d02MXT1T8nk8V1aSH7IOnmxgcAAAAAIoW96SXAifOnZBFFhkyZJFFzao1U/uQ9rJYLLJarNd87T+7X6t+XOU+tn+z/mpSrYnr+xmZsh45Kuuhw67X/07I6jS0q5r0bivJ6pScVunJrVLb/0l+DtfL1/nrn1d8vamuNLKX3Dvi7+6oLXu/v0mRkVLjxpLVes3zsw+eLrso5wAAAADwW5R0DzXo9kFasXuFbBabHIZD07tMv/n7yxdPVPzu/ygquJ3smW2lDzZJmzZJSUlXT65XTyobpt7L1iu+nkVRhwzZn5wrdelyw/e5dd061X595OXj/jFXsvOxaAAAAABQUFzu7sFik2Jv/jPBnU5p1y5pzhzp7bevPe/WW1073Zdeder8+maxUny8FBWVv6Jd0OMAAAAAwEvkp4eaXtLnzp2rl19+WSdPnlSzZs00a9YsRUZGXnP+hg0bNHbsWO3atUvBwcEaP368RowYcdPvV5JK+nVlZUk7drh2yDdtkjZvln7++ep5NWtKgwe7CnmHDlKVKsWfFQAAAAC8WIm5Jz0mJkajR4/W3Llz1aFDB7399tu65557tHv3btWtW/eq+YcOHVLPnj316KOP6oMPPtCXX36pkSNHqlq1arr//vtNOINidO6c9NVXl0v5V19J58/nnhMQIDVsKH3/vet+cKfTtavODjcAAAAAlAim7qS3bdtWERERmjdvnnusSZMmuu+++zRjxoyr5v/1r39VbGys9uzZ4x4bMWKEvvvuO23duvWm3rPE7KSfPSu98or02WdSWpp06JDkcOSeExQk3Xnn5UvXW7Z0PUGdS9ABAAAAwGOUiJ30rKwsJSQkaMKECbnGu3Xrpi1btuR5zNatW9WtW7dcY927d9f8+fOVnZ0t3998xNclFy9e1MWLF91fp6WlFUL6YjBunLRgQe6xunVz309+rSeo2+2UcwAAAAAogUwr6SkpKXI4HKpRo0au8Ro1aig5OTnPY5KTk/Ocn5OTo5SUFNWqVeuqY2bMmKGpU6cWXvDi8ttL2a1W6eGHpXffNS8PAAAAAKDIXfuDrIuJxWLJ9bVhGFeN3Wh+XuOXPPPMM0pNTXW/jh079jsTF5MHH3T9abO57i1nZxwAAAAASj3TdtKrVq0qm8121a756dOnr9otv6RmzZp5zvfx8VFQUFCex/j7+8vf379wQhcnu11avZp7ywEAAADAi5i2k+7n56dWrVopLi4u13hcXJzat2+f5zHt2rW7av7nn3+u1q1b53k/eolnt0szZ1LQAQAAAMBLmHq5+9ixY/Xuu+/qvffe0549ezRmzBgdPXrU/bnnzzzzjIYMGeKeP2LECB05ckRjx47Vnj179N5772n+/PkaN26cWacAAAAAAEChMfVz0qOjo3XmzBlNmzZNJ0+e1K233qq1a9cqNDRUknTy5EkdPXrUPb9evXpau3atxowZozlz5ig4OFizZ88u/Z+RDgAAAADwCqZ+TroZSsznpAMAAAAASoX89FDTn+4OAAAAAABcKOkAAAAAAHgISjoAAAAAAB6Ckg4AAAAAgIegpAMAAAAA4CEo6QAAAAAAeAhKOgAAAAAAHoKSDgAAAACAh6CkAwAAAADgISjpAAAAAAB4CEo6AAAAAAAegpIOAAAAAICHoKQDAAAAAOAhfMwOUNwMw5AkpaWlmZwEAAAAAOANLvXPS330eryupKenp0uSQkJCTE4CAAAAAPAm6enpCgwMvO4ci3EzVb4UcTqdOnHihCpUqCCLxWJ2nOtKS0tTSEiIjh07pooVK5odB7gKaxSejjUKT8cahadjjcLTlZQ1ahiG0tPTFRwcLKv1+nede91OutVqVZ06dcyOkS8VK1b06AUHsEbh6Vij8HSsUXg61ig8XUlYozfaQb+EB8cBAAAAAOAhKOkAAAAAAHgISroH8/f31+TJk+Xv7292FCBPrFF4OtYoPB1rFJ6ONQpPVxrXqNc9OA4AAAAAAE/FTjoAAAAAAB6Ckg4AAAAAgIegpAMAAAAA4CEo6QAAAAAAeAhKuonmzp2revXqqUyZMmrVqpU2bdp03fkbNmxQq1atVKZMGdWvX19vvfVWMSWFN8vPOv3oo4909913q1q1aqpYsaLatWunzz77rBjTwhvl93fpJV9++aV8fHzUokWLog0Ir5ffNXrx4kVNnDhRoaGh8vf3V4MGDfTee+8VU1p4o/yu0SVLlqh58+YKCAhQrVq19PDDD+vMmTPFlBbeZuPGjerdu7eCg4NlsVj08ccf3/CYkt6bKOkmiYmJ0ejRozVx4kQlJiYqMjJS99xzj44ePZrn/EOHDqlnz56KjIxUYmKinn32WY0aNUoffvhhMSeHN8nvOt24caPuvvturV27VgkJCerUqZN69+6txMTEYk4Ob5HfNXpJamqqhgwZoi5duhRTUnirgqzRfv36ad26dZo/f76SkpK0bNkyNW7cuBhTw5vkd41u3rxZQ4YM0bBhw7Rr1y6tWLFCX3/9tR555JFiTg5vkZGRoebNm+vNN9+8qfmlojcZMEWbNm2MESNG5Bpr3LixMWHChDznjx8/3mjcuHGusccee8y44447iiwjkN91mpemTZsaU6dOLexogGEYBV+j0dHRxnPPPWdMnjzZaN68eREmhLfL7xr9z3/+YwQGBhpnzpwpjnhAvtfoyy+/bNSvXz/X2OzZs406deoUWUbgEknGqlWrrjunNPQmdtJNkJWVpYSEBHXr1i3XeLdu3bRly5Y8j9m6detV87t3765vvvlG2dnZRZYV3qsg6/RKTqdT6enpqlKlSlFEhJcr6BpdsGCBDhw4oMmTJxd1RHi5gqzR2NhYtW7dWv/4xz9Uu3ZtNWrUSOPGjdP58+eLIzK8TEHWaPv27XX8+HGtXbtWhmHo1KlTWrlypf74xz8WR2TghkpDb/IxO4A3SklJkcPhUI0aNXKN16hRQ8nJyXkek5ycnOf8nJwcpaSkqFatWkWWF96pIOv0Sq+++qoyMjLUr1+/oogIL1eQNbpv3z5NmDBBmzZtko8P/wpE0SrIGj148KA2b96sMmXKaNWqVUpJSdHIkSN19uxZ7ktHoSvIGm3fvr2WLFmi6OhoXbhwQTk5ObLb7XrjjTeKIzJwQ6WhN7GTbiKLxZLra8Mwrhq70fy8xoHClN91esmyZcs0ZcoUxcTEqHr16kUVD7jpNepwODRgwABNnTpVjRo1Kq54QL5+jzqdTlksFi1ZskRt2rRRz549NXPmTC1cuJDddBSZ/KzR3bt3a9SoUZo0aZISEhL06aef6tChQxoxYkRxRAVuSknvTWwjmKBq1aqy2WxX/RfK06dPX/VffS6pWbNmnvN9fHwUFBRUZFnhvQqyTi+JiYnRsGHDtGLFCnXt2rUoY8KL5XeNpqen65tvvlFiYqL+/Oc/S3IVIsMw5OPjo88//1ydO3culuzwDgX5PVqrVi3Vrl1bgYGB7rEmTZrIMAwdP35ct9xyS5FmhncpyBqdMWOGOnTooKefflqSdPvtt6tcuXKKjIzUCy+8UCJ2KVG6lYbexE66Cfz8/NSqVSvFxcXlGo+Li1P79u3zPKZdu3ZXzf/888/VunVr+fr6FllWeK+CrFPJtYP+0EMPaenSpdyfhiKV3zVasWJF7dy5U99++637NWLECIWHh+vbb79V27Ztiys6vERBfo926NBBJ06c0Llz59xje/fuldVqVZ06dYo0L7xPQdZoZmamrNbcFcJms0m6vFsJmKlU9CaTHljn9ZYvX274+voa8+fPN3bv3m2MHj3aKFeunHH48GHDMAxjwoQJxuDBg93zDx48aAQEBBhjxowxdu/ebcyfP9/w9fU1Vq5cadYpwAvkd50uXbrU8PHxMebMmWOcPHnS/frll1/MOgWUcvldo1fi6e4oavldo+np6UadOnWMvn37Grt27TI2bNhg3HLLLcYjjzxi1imglMvvGl2wYIHh4+NjzJ071zhw4ICxefNmo3Xr1kabNm3MOgWUcunp6UZiYqKRmJhoSDJmzpxpJCYmGkeOHDEMo3T2Jkq6iebMmWOEhoYafn5+RkREhLFhwwb394YOHWp07Ngx1/z4+HijZcuWhp+fnxEWFmbMmzevmBPDG+VnnXbs2NGQdNVr6NChxR8cXiO/v0t/i5KO4pDfNbpnzx6ja9euRtmyZY06deoYY8eONTIzM4s5NbxJftfo7NmzjaZNmxply5Y1atWqZQwcONA4fvx4MaeGt1i/fv11//9laexNFsPguhQAAAAAADwB96QDAAAAAOAhKOkAAAAAAHgISjoAAAAAAB6Ckg4AAAAAgIegpAMAAAAA4CEo6QAAAAAAeAhKOgAAAAAAHoKSDgAAAACAh6CkAwDgZRYuXKhKlSqZHUPx8fGyWCz65ZdfzI4CAIDHoKQDAGAyi8Vy3ddDDz1U4J8dFhamWbNm5RqLjo7W3r17f1/oG4iKirruOYWFhal9+/Y6efKkAgMDizQLAAAliY/ZAQAA8HYnT550/z0mJkaTJk1SUlKSe6xs2bKF+n5ly5Yt9J95pY8++khZWVmSpGPHjqlNmzb64osv1KxZM0mSzWaTn5+fatasWaQ5AAAoadhJBwDAZDVr1nS/AgMDZbFYco1t3LhRrVq1UpkyZVS/fn1NnTpVOTk57uOnTJmiunXryt/fX8HBwRo1apQk1272kSNHNGbMGPcOtnT15e5TpkxRixYttHjxYoWFhSkwMFD9+/dXenq6e056eroGDhyocuXKqVatWnrttdcUFRWl0aNH53lOVapUceevVq2aJCkoKCjX2JWXu1/KtWbNGoWHhysgIEB9+/ZVRkaGFi1apLCwMFWuXFl/+ctf5HA43O+VlZWl8ePHq3bt2ipXrpzatm2r+Pj4QvhfBgCA4sdOOgAAHuyzzz7ToEGDNHv2bEVGRurAgQMaPny4JGny5MlauXKlXnvtNS1fvlzNmjVTcnKyvvvuO0mu3ezmzZtr+PDhevTRR6/7PgcOHNDHH3+sNWvW6Oeff1a/fv300ksvafr06ZKksWPH6ssvv1RsbKxq1KihSZMmaceOHWrRokWhnm9mZqZmz56t5cuXKz09XX369FGfPn1UqVIlrV27VgcPHtT999+vO++8U9HR0ZKkhx9+WIcPH9by5csVHBysVatWqUePHtq5c6duueWWQs0HAEBRo6QDAODBpk+frgkTJmjo0KGSpPr16+v555/X+PHjNXnyZB09elQ1a9ZU165d5evrq7p166pNmzaSXLvZNptNFSpUuOFl5U6nUwsXLlSFChUkSYMHD9a6des0ffp0paena9GiRVq6dKm6dOkiSVqwYIGCg4ML/Xyzs7M1b948NWjQQJLUt29fLV68WKdOnVL58uXVtGlTderUSevXr1d0dLQOHDigZcuW6fjx4+4848aN06effqoFCxboxRdfLPSMAAAUJUo6AAAeLCEhQV9//bV7R1uSHA6HLly4oMzMTD3wwAOaNWuW6tevrx49eqhnz57q3bu3fHzy96/4sLAwd0GXpFq1aun06dOSpIMHDyo7O9td/iUpMDBQ4eHhv/PsrhYQEOAu6JJUo0YNhYWFqXz58rnGLmXbsWOHDMNQo0aNcv2cixcvKigoqNDzAQBQ1CjpAAB4MKfTqalTp6pPnz5Xfa9MmTIKCQlRUlKS4uLi9MUXX2jkyJF6+eWXtWHDBvn6+t70+1w512KxyOl0SpIMw3CP/dal8cKUV47rZXM6nbLZbEpISJDNZss177fFHgCAkoKSDgCAB4uIiFBSUpIaNmx4zTlly5aV3W6X3W7XE088ocaNG2vnzp2KiIiQn59froesFUSDBg3k6+ur7du3KyQkRJKUlpamffv2qWPHjr/rZ/9eLVu2lMPh0OnTpxUZGWlqFgAACgMlHQAADzZp0iT16tVLISEheuCBB2S1WvX9999r586deuGFF7Rw4UI5HA61bdtWAQEBWrx4scqWLavQ0FBJrsvYN27cqP79+8vf319Vq1bNd4YKFSpo6NChevrpp1WlShVVr15dkydPltVqvWp3vbg1atRIAwcO1JAhQ/Tqq6+qZcuWSklJ0X//+1/ddttt6tmzp6n5AADILz6CDQAAD9a9e3etWbNGcXFx+sMf/qA77rhDM2fOdJfwSpUq6Z133lGHDh10++23a926dfrkk0/c92NPmzZNhw8fVoMGDdwfhVYQM2fOVLt27dSrVy917dpVHTp0UJMmTVSmTJlCOc/fY8GCBRoyZIieeuophYeHy263a9u2be5dfwAAShKLURQ3lAEAgFItIyNDtWvX1quvvqphw4aZHQcAgFKDy90BAMANJSYm6scff1SbNm2UmpqqadOmSZLuvfdek5MBAFC6UNIBAMBNeeWVV5SUlCQ/Pz+1atVKmzZtKtA97gAA4Nq43B0AAAAAAA/Bg+MAAAAAAPAQlHQAAAAAADwEJR0AAAAAAA9BSQcAAAAAwENQ0gEAAAAA8BCUdAAAAAAAPAQlHQAAAAAAD0FJBwAAAADAQ/w/sJFqyxcXwVgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, LSTM\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import srdata\n",
    "\n",
    "# 1: シミュレーション\n",
    "    # 1: 指数分布モデル\n",
    "    # 2: ガンマ分布モデル\n",
    "    # 3: 切断正規分布モデル\n",
    "# 2: 実データ\n",
    "    # 1: Lyu/J1.csv\n",
    "    # 2: Lyu/J2.csv\n",
    "    # 3: Lyu/J3.csv\n",
    "    # 4: Lyu/J4.csv\n",
    "    # 5: Lyu/J5.csv\n",
    "data_type = 2\n",
    "dataset_index = 1\n",
    "\n",
    "# 学習データとテストデータの割合（全体のデータ数ににtrain_ratioをかけて小数点以下切り捨てした数が学習データ数）\n",
    "train_ratio = 0.8\n",
    "\n",
    "# ハイパーパラメータ\n",
    "hidden_units = 500\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# 正規化の有無\n",
    "normalization = True\n",
    "\n",
    "# 最良モデルの保存\n",
    "save_best_model = True\n",
    "\n",
    "# 早期終了の有無（未実装）\n",
    "early_stopping = False\n",
    "\n",
    "# 早期終了のパラメータ\n",
    "# patience = 10\n",
    "\n",
    "# 学習データの重みづけの有無\n",
    "sample_weight = True\n",
    "\n",
    "# 学習のverbose\n",
    "verbose = 1\n",
    "\n",
    "# 追加の予測値\n",
    "additional_prediction = False\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "def generate_data(data_type, dataset_index, size=100, scale=50, shape=2, loc=0):\n",
    "    # シミュレーション\n",
    "    if data_type == 1:\n",
    "        # シードの固定\n",
    "        seed = 1\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # 指数分布モデル\n",
    "        if dataset_index == 1:\n",
    "            # データの生成\n",
    "            samples = np.random.exponential(scale, size)\n",
    "            sorted_samples = np.sort(samples)\n",
    "\n",
    "            # データの保存\n",
    "            with open(\"datasets/simulaton/exponential.csv\", \"w\") as f:\n",
    "                for sample in sorted_samples:\n",
    "                    f.write(str(sample) + \"\\n\")\n",
    "\n",
    "        # ガンマ分布モデル\n",
    "        # 正式な分布と違う？(https://github.com/SwReliab/SRATS2017/blob/master/docs/pdfs/srats_model.pdf)\n",
    "        # こっちがあっているかも(https://www.researchgate.net/profile/Hiroyuki-Okamura/publication/261238504_SRATS_Software_reliability_assessment_tool_on_spreadsheet_Experience_report/links/5788308e08ae21394a0c7b1e/SRATS-Software-reliability-assessment-tool-on-spreadsheet-Experience-report.pdf)\n",
    "        # 将来的に逆変換サンプリングを用いて実装する予定\n",
    "        elif dataset_index == 2:\n",
    "            # データの生成\n",
    "            samples = np.random.gamma(shape, scale, size)\n",
    "            sorted_samples = np.sort(samples)\n",
    "\n",
    "            # データの保存\n",
    "            with open(\"datasets/simulaton/gamma.csv\", \"w\") as f:\n",
    "                for sample in sorted_samples:\n",
    "                    f.write(str(sample) + \"\\n\")\n",
    "\n",
    "        # 切断正規分布モデル\n",
    "        # 正式な分布と違う？(https://github.com/SwReliab/SRATS2017/blob/master/docs/pdfs/srats_model.pdf)\n",
    "        elif dataset_index == 3:\n",
    "            # データの生成\n",
    "            truncnorm_dist = truncnorm((0 - loc) / scale, (np.inf - 0) / scale, loc=loc, scale=scale)\n",
    "            samples = truncnorm_dist.rvs(size)\n",
    "            sorted_samples = np.sort(samples)\n",
    "\n",
    "            # データの保存\n",
    "            with open(\"datasets/simulaton/truncnorm.csv\", \"w\") as f:\n",
    "                for sample in sorted_samples:\n",
    "                    f.write(str(sample) + \"\\n\")\n",
    "\n",
    "        # データの作成\n",
    "        X = sorted_samples.reshape(-1, 1)\n",
    "        y = np.arange(1, len(sorted_samples) + 1).reshape(-1, 1)\n",
    "\n",
    "    # 実データ\n",
    "    elif data_type == 2:\n",
    "        dataset_names = [\"Lyu/J1.csv\", \"Lyu/J2.csv\", \"Lyu/J3.csv\", \"Lyu/J4.csv\", \"Lyu/J5.csv\"]\n",
    "        data_df = srdata.get_dataset(dataset_names[dataset_index - 1])\n",
    "        data = data_df.iloc[:, 0].values\n",
    "        cum_data = data.cumsum()\n",
    "        X = np.arange(1, len(cum_data) + 1).reshape(-1, 1)\n",
    "        y = cum_data.reshape(-1, 1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "# tensorflowのランダムシードの固定\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "# データの生成\n",
    "X, y = generate_data(data_type, dataset_index)\n",
    "print(X)\n",
    "\n",
    "# データの分割\n",
    "train_size = int(len(X) * train_ratio)  # 小数点以下切り捨て\n",
    "\n",
    "# 正規化\n",
    "if normalization:\n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y)\n",
    "\n",
    "# データの分割\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "# モデルの構築（Functional API）\n",
    "input_layer = Input(shape=(1,))\n",
    "x = Dense(hidden_units, activation=\"relu\")(input_layer)\n",
    "x = Dense(hidden_units, activation=\"relu\")(x)\n",
    "x = Dense(hidden_units, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "output_layer = Dense(1, use_bias=False, kernel_initializer=Constant(value=1.0))(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# モデルのサマリーをテキストファイルに保存\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    # 標準出力を一時的にファイルにリダイレクト\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "\n",
    "    # モデルのサマリーを出力\n",
    "    model.summary()\n",
    "\n",
    "    # 標準出力を元に戻す\n",
    "    sys.stdout = old_stdout\n",
    "\n",
    "# ModelCheckpointの設定\n",
    "callbacks_list = []\n",
    "if save_best_model:\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list.append(checkpoint)\n",
    "\n",
    "# サンプルごとの重み\n",
    "sample_weight_array = None\n",
    "if sample_weight:\n",
    "    sample_weight_array = np.array([i for i in range(1, len(X_train) + 1)])\n",
    "\n",
    "# 学習\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, sample_weight=sample_weight_array, callbacks=callbacks_list)\n",
    "\n",
    "if (save_best_model):\n",
    "    model = load_model('best_model.h5')\n",
    "    min_loss_index = np.argmin(history.history['loss'])\n",
    "    print(\"Best model is from epoch {} and loss is {}\".format(min_loss_index + 1, history.history['loss'][min_loss_index]))\n",
    "\n",
    "# パラメータを保存\n",
    "# h5形式で保存\n",
    "model.save_weights('weights/weights.h5')\n",
    "# テキスト形式で保存\n",
    "weights = model.get_weights()\n",
    "with open('weights/model_weights.txt', 'w') as f:\n",
    "    for layer_weights in weights:\n",
    "        np.savetxt(f, layer_weights, fmt='%s')\n",
    "\n",
    "# 損失関数をプロット（横軸：エポック，縦軸：損失関数）\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "if MinMaxScaler:\n",
    "    plt.ylim(0, 0.01)   # 正規化する場合ylimを制限しないと損失関数が小さくなりすぎて見えなくなる\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 予測（学習データも含む）\n",
    "pred_X = X\n",
    "if additional_prediction and normalization:\n",
    "    pred_X = np.append(pred_X, np.linspace(1, 2, 10).reshape(-1, 1))\n",
    "predictions = model.predict(pred_X)\n",
    "\n",
    "# 推定フォールト数の表示\n",
    "if normalization:\n",
    "    print(\"Predicted number of faults: {}\".format(scaler_X.inverse_transform(predictions)[-1]))\n",
    "else :\n",
    "    print(\"Predicted number of faults: {}\".format(predictions[-1]))\n",
    "\n",
    "# 予測値のプロット\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(X_train, y_train, color=\"red\", label='True_train', marker='o', markersize=2)\n",
    "plt.plot(X_test, y_test, color=\"blue\", label='True_test', marker='o', markersize=2)\n",
    "plt.plot(pred_X, predictions, color=\"green\", label='Predicted', marker='o', markersize=2)\n",
    "plt.title('Prediction')\n",
    "plt.xlabel('Testing Time')\n",
    "plt.ylabel('Cumulative Number of Failures')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
